\newpage
\thispagestyle{plain}
~\\
\vfill
{ 
	\subsection*{Authors}
	Jakob Huber <jhuber@kth.se>\\
	School of Engineering Sciences\\
	KTH, \textit{Royal Institute of Technology}
	
	\subsection*{Place for Project}
	\textbf{KTH} SCI\\
	SE-100 44 Stockholm, Sweden

	\subsection*{Examiner}
	Jack Lidmar <jlidmar@kth.se>\\
	Department for Theoretical Physics \\
	KTH, \textit{Royal Institute of Technology}
	
	\subsection*{Supervisor KTH}
	Prof. Egor Babaev <babaev@kth.se>\\
	and \\
	Emil Blomquist <emilbl@kth.se>\\
	Department for Theoretical Physics\\
	KTH, \textit{Royal Institute of Technology}


	\subsection*{Supervisor Company}
	Dr. Siril Yella \\
	Rumbline Consulting AB
}


\newpage
\thispagestyle{plain}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  The English abstract          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The challenge of overfitting is explored by introducing a regularization scheme with simultaneous structural evolution for a neural network in training. Similar to regular pruning with cross-validation, a novel statistical physics-inspired algorithm is proposed. The regularization consists of generating perturbations of the neural net structure and randomly adopting among the most, on a selection set, positive alterations. The allowed perturbations consist of duplicating or pruning nodes and pruning weights. A proof-of-concept is sought for the algorithm with a fully connected RNN and tested on a supervised learning task using the MNIST data set. The algorithm is shown to benefit small to medium-sized networks on the tested data set and diminishes the hidden layer size's overall effect on test set performance. With proper scaling, the proposed algorithm could benefit neural networks with arbitrary structure.


\subsection*{Keywords}
regularization, machine learning, AI, statistical physics, neural networks, deep learning


\newpage
\thispagestyle{plain}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%	 The Swedish abstract         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Utmaningen med överanpassning utforskas genom att införa ett system för normalisering med samtidig strukturell utveckling för ett neuralt nätverk under träning. I likhet med vanlig beskärning med korsvalidering föreslås en ny statistisk fysikinspirerad algoritm. Regulariseringen består av att generera störningar i den neurala nätstrukturen och slumpmässigt använda positiva förändringar bland de mest, på en urvalsuppsättning. De tillåtna störningarna består av duplicering eller beskärning av noder och beskärningsvikter. Ett bevis på koncept eftersträvas för algoritmen med en fullständigt ansluten RNN och testas på en övervakad inlärningsuppgift med MNIST-datauppsättningen. Algoritmen har visat sig gynna små till medelstora nätverk på den testade datamängden och minskar den dolda lagrets storleks totala effekt på testuppsättningens prestanda. Med korrekt skalning kan den föreslagna algoritmen gynna neurala nätverk med godtycklig struktur.

\subsection*{Nyckelord}
regularisering, maskininlärning, AI, statistisk fysik, neurala nätverk, djup maskininlärning


\newpage
\thispagestyle{plain}
\chapter*{Acknowledgements}
I would like to express my thanks to my supervisor Prof. Egor Babaev at KTH. Dr. Siril Yella at Rumbline Consulting for structure and feedback on my report. Per Ininbergs at Rumbline Consulting for support and motivation. Wile Balkhed and Viktor Nilsson for supervising in general time management and thesis writing. 

Mostly I would like to thank my supervisor Emil Blomquist at KTH for giving me this oppurtunity and the hard work put in to start-off and explore in a new direction.

\newpage

\thispagestyle{plain}

\tableofcontents

\newpage
\pagenumbering{arabic}

