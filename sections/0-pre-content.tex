\newpage
\thispagestyle{plain}
~\\
\vfill
{ 
	\subsection*{Authors}
	Jakob Huber <jhuber@kth.se>\\
	School of Engineering Sciences\\
	KTH, \textit{Royal Institute of Technology}
	
	\subsection*{Place for Project}
	\textbf{KTH} SCI\\
	SE-100 44 Stockholm, Sweden

	\subsection*{Examiner}
	Jack Lidmar <jlidmar@kth.se>\\
	Department for Theoretical Physics \\
	KTH, \textit{Royal Institute of Technology}
	
	\subsection*{Supervisor KTH}
	Prof. Egor Babaev <babaev@kth.se>\\
	Department for Theoretical Physics\\
	KTH, \textit{Royal Institute of Technology}


	\subsection*{Supervisor Company}
	Dr. Siril Yella \\
	Rumbline Consulting AB
}


\newpage
\thispagestyle{plain}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  The English abstract          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Statistical models bear the inherent problem of overfitting, consisting of more parameters than justifiable based on the data. For artificial neural networks, computing abstractions of biological neural networks, the parameters consist of artificial neurons and connections. The challenge of overfitting is explored by introducing a regularization scheme by a dynamical structural evolution for an artificial neural network in training. A novel statistical physics-inspired algorithm is proposed by inserting or deleting parameters by cross-validation on a selection data set. The regularization consists of generating perturbations of the neural net structure and randomly adopting among the most positive alterations. These operations are formalized to allow for generalization and application on any neural net set-up. A proof-of-concept is sought for the algorithm with a fully connected recurrent neural network and tested on a supervised learning task using the MNIST data set. The algorithm is shown to benefit small to medium-sized networks on the tested data set and diminishes the hidden layer size's overall effect on test set performance. With proper scaling, the proposed algorithm could benefit artificial neural networks with arbitrary structure and size.


\subsection*{Keywords}
regularization, machine learning, AI, statistical physics, neural networks, deep learning


\newpage
\thispagestyle{plain}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%	 The Swedish abstract         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Statistiska modeller har det inneboende problemet med överanpassning, de kan bestå av fler parametrar än motiverat baserat på data. För artificiella neurala nätverk, datorabstraktioner av biologiska neurala nätverk, består parametrarna av artificiella nervceller och anslutningar. Utmaningen med överanpassning utforskas genom att införa ett system för regularisering genom en dynamisk strukturell utveckling för ett artificiellt neuralt nätverk under träning. En ny statistisk fysikinspirerad algoritm föreslås genom att infoga eller ta bort parametrar genom korsvalidering på en urvalsdatauppsättning. Regulariseringen består i att generera störningar i den neurala nätstrukturen och slumpmässigt anta en bland de mest positiva förändringarna. Dessa operationer är formaliserade för att möjliggöra generalisering och tillämpning på alla neurala nätuppsättningar. Ett bevis på koncept eftersträvas för algoritmen på ett fullt anslutet återkommande neuralt nätverk och testas på en övervakad inlärningsuppgift på MNIST-datauppsättningen. Algoritmen har visat sig gynna små till medelstora nätverk på den testade datamängden och minskar det dolda lagrets storleks totala effekt på testuppsättningens prestanda. Med korrekt skalning kan den föreslagna algoritmen gynna artificiella neurala nätverk med godtycklig struktur och storlek.

\subsection*{Nyckelord}
regularisering, maskininlärning, AI, statistisk fysik, neurala nätverk, djup maskininlärning


\newpage
\thispagestyle{plain}
\chapter*{Acknowledgements}
I want to express thanks to my supervisor Prof. Egor Babaev at KTH, for providing the problem of this thesis and supervision. Dr. Siril Yella at Rumbline Consulting for structure and feedback on my report. Per Ininbergs at Rumbline Consulting for support and motivation in what comes after the thesis. Wile Balkhed and Viktor Nilsson for reminding of the importance of general time management and thesis writing. 

I would most like to thank my supervisor Emil Blomquist at KTH for supervising and collaborating. Also, giving me this opportunity and the hard work put into start-off and exploring in a new direction.

\newpage

\thispagestyle{plain}

\tableofcontents

\newpage
\pagenumbering{arabic}

