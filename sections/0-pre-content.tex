\newpage
\thispagestyle{plain}
~\\
\vfill
{ 
	\subsection*{Authors}
	Jakob Huber <jhuber@kth.se>\\
	School of Engineering Sciences\\
	KTH, \textit{Royal Institute of Technology}
	
	\subsection*{Place for Project}
	\textbf{KTH} SCI\\
	SE-100 44 Stockholm, Sweden

	\subsection*{Examiner}
	Jack Lidmar <jlidmar@kth.se>\\
	Department for Theoretical Physics \\
	KTH, \textit{Royal Institute of Technology}
	
	\subsection*{Supervisor KTH}
	Prof. Egor Babaev <babaev@kth.se>\\
	and \\
	Emil Blomquist <emilbl@kth.se>\\
	Department for Theoretical Physics\\
	KTH, \textit{Royal Institute of Technology}


	\subsection*{Supervisor Company}
	Dr. Siril Yella \\
	Rumbline Consulting AB
}


\newpage
\thispagestyle{plain}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  The English abstract          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The challenge of overfitting is explored by introducing a regularization scheme with simultaneous structural evolution for a neural network in training. Similar to regular pruning with cross-validation, a novel statistical physics-inspired algorithm is proposed. The regularization consists of generating perturbations of the neural net structure and randomly adopting among the most, on a selection set, positive alterations. The allowed perturbations consist of duplicating or pruning nodes and pruning weights. A proof-of-concept is sought for the algorithm with a fully connected RNN and tested on a supervised learning task using the MNIST data set. The algorithm is shown to benefit small to medium-sized networks on the tested data set and diminishes the hidden layer size's overall effect on test set performance. With proper scaling, the proposed algorithm could benefit neural networks with arbitrary structure and size.


\subsection*{Keywords}
regularization, machine learning, AI, statistical physics, neural networks, deep learning


\newpage
\thispagestyle{plain}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%	 The Swedish abstract         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Utmaningen med överanpassning utforskas genom att införa ett system för regularisering med samtidig strukturell utveckling av ett neuralt nätverk under träning. I likhet med vanlig beskärning av nätverk med korsvalidering föreslås en ny statistisk fysik-inspirerad algoritm. Regulariseringen består av att generera störningar i den neurala nätstrukturen och slumpmässigt anta, bland de på en urvalsuppsättning mest fördelaktiga, förändringarna. Formellt behandlas de operationerna på grafer. De tillåtna störningarna består av duplicering eller beskärning av noder och beskärning av vikter. Ett koncepttest eftersträvas för algoritmen på en fullständigt kopplad RNN och testas på en övervakad inlärningsuppgift med MNIST-datauppsättning. Algoritmen har visat sig gynna små till medelstora nätverk på den testade datamängden och minskar det dolda lagrets storleks totala effekt på prestanda vid test på testdatauppsättningen. Med korrekt skalning kan den föreslagna algoritmen gynna neurala nätverk med godtycklig struktur och storlek.

\subsection*{Nyckelord}
regularisering, maskininlärning, AI, statistisk fysik, neurala nätverk, djup maskininlärning


\newpage
\thispagestyle{plain}
\chapter*{Acknowledgements}
I would like to express my thanks to my supervisor Prof. Egor Babaev at KTH. Dr. Siril Yella at Rumbline Consulting for structure and feedback on my report. Per Ininbergs at Rumbline Consulting for support and motivation. Wile Balkhed and Viktor Nilsson for supervising in general time management and thesis writing. 

Mostly I would like to thank my supervisor Emil Blomquist at KTH for giving me this oppurtunity and the hard work put in to start-off and explore in a new direction.

\newpage

\thispagestyle{plain}

\tableofcontents

\newpage
\pagenumbering{arabic}

