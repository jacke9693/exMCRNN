
\chapter{Introduction}

In the introduction, first comes the thesis's aim, then a presentation of the artificial neural network is included. The more ground treatment is due to a background in physics rather than data science. The next necessary section is a deep-dive into the deep learning field; thus, a literary study is included to set a proper context for recurrent neural networks and their history. Then a brief section of regularization and the different methods used in conjunction with neural networks training. Motivation is the section after that with a record of how the Monte Carlo Regularization method came to be. Lastly, related work to the regularization specifically is included.

\section{Aim}

In this master thesis, the prospect of a recurrent neural network (RNN) with a stochastic structure varying in time to regularize it, Monte Carlo regularization (MCR) is presented. A regularization technique is introduced to combat overfitting for the neural network. Inspiration is drawn from biology, where neural networks organically evolve their structure, formalized in the concept of a Markov process. However, how this algorithm is structured is inspired by the physics processes of grand-canonical ensembles or processes where energy differences drive the algorithm — evaluating the energy differences of either pruning or duplication of neurons, thus regulating the structure of the neural net. The regularization is tested on an RNN. Their recurrent structure could strain this process of regularization and prove insightful. 

\section{Neural networks}

Artificial neural networks consider the elements of propagation and the structure of biological neural networks. An input generates a signal sent through the network from layer to layer, which consists of nodes, corresponding to neurons. The layers vary in size with the application or data analyzed. The connections between the nodes correspond to synapses; dependencies allowed give rise to different neural networks. Both nodes and connections pertain to a strength, denoted bias, and weight, respectively. These biases and weights establish the parameters of the artificial neural network. 

In a standard artificial feed-forward neural network, where signals proceed without recurrence, there is an input, a hidden and an output layer of nodes, see Fig. \ref{feed}. In Fig. \ref{feed}, the relation to directed acyclic graphs is illustrated. The data set and related task determine the input and output layer; the hidden layer is the computational one, and often one is enough. This result is due to the universal approximation theorem. The processing of input and subsequent signal propagation creates a response that can read as output. The output can then be compared to a target output; the discrepancy between the two then constitutes an error. The error is then sent back through the network and utilized to adjust parameters; after a sufficient successive amount of such trials, the neural network is trained. 

\begin{figure}[h]
    \centering
    \textbf{A representation of a neural network as a graph}\par\medskip
    \begin{neuralnetwork}[height=4]
        \newcommand{\x}[2]{$x_#2$}
        \newcommand{\y}[2]{$\hat{y}_#2$}
        \newcommand{\hfirst}[2]{\small $h^{(1)}_#2$}
        \newcommand{\hsecond}[2]{\small $h^{(2)}_#2$}
        \inputlayer[count=2, bias=true, title=Input\\layer, text=\x]
        \hiddenlayer[count=4, bias=false, title=Hidden\\layer 1, text=\hfirst] \linklayers
        \outputlayer[count=2, title=Output\\layer, text=\y] \linklayers
    \end{neuralnetwork}
    \caption{A feedforward network, with a single hidden layer. The input proceeds from left to right in the directed graph.}
    \label[fig]{feed}
\end{figure}

Allowing for additional hidden layers approaches deep learning. Another way is to allow for recursion in the hidden layer. Such modification relates to the specialization of the network to more complicated tasks.


\section{Recurrent Neural Network - History}

RNNs are a class derived from feed-forward neural networks with iteration over a fixed structure to achieve temporal dynamics \cite{DRNNS}, see Fig. \ref{rnn}. Forming along the time axis is a directed graph, achieving dynamic behavior in their internal state in the sense of memory. Utilizing the memory to capture patterns also requires dynamical programming for the necessity of mapping long sequences to short or vice versa. The RNNs excel at pattern recognition in data sequences, e.g., natural language processing, machine translation, and, most commonly, speech recognition \cite{handwriting}. 

\begin{figure}[h]
    \centering
    \textbf{Illustration of a recurrent neural network unfolded}\par\medskip
\begin{tikzpicture}[item/.style={circle,draw,thick,align=center},
    itemc/.style={item,on chain,join}]
     \begin{scope}[start chain=going right,nodes=itemc,every
     join/.style={-latex,very thick},local bounding box=chain]
     \path node (A0) {$H$} node (A1) {$H$} node (A2) {$H$} node[xshift=2em] (At)
     {$H$};
     \end{scope}
     \node[left=1em of chain,scale=2] (eq) {$=$};
     \node[left=2em of eq,item] (AL) {$H$};
     \path (AL.west) ++ (-1em,2em) coordinate (aux);
     \draw[very thick,-latex,rounded corners] (AL.east) -| ++ (1em,2em) -- (aux) 
     |- (AL.west);
     \foreach \X in {0,1,2,t} 
     {\draw[very thick,-latex] (A\X.north) -- ++ (0,2em)
     node[above,item,fill=gray!10] (h\X) {$o_\X$};
     \draw[very thick,latex-] (A\X.south) -- ++ (0,-2em)
     node[below,item,fill=gray!10] (x\X) {$i_\X$};}
     \draw[white,line width=0.8ex] (AL.north) -- ++ (0,1.9em);
     \draw[very thick,-latex] (AL.north) -- ++ (0,2em)
     node[above,item,fill=gray!10] {$o_t$};
     \draw[very thick,latex-] (AL.south) -- ++ (0,-2em)
     node[below,item,fill=gray!10] {$i_t$};
     \path (x2) -- (xt) node[midway,scale=2,font=\bfseries] {\dots};
\end{tikzpicture}
\caption{An unfolded recurrent neural network's flow diagram with time dependent input $i$ and output $o$.}
\label[fig]{rnn}
\end{figure}

Initially, RNNs were called dynamic recurrent neural networks, DRNs, displaying time-varying responses as per dynamical systems. During the early stages of development, a vital issue was shown by Bengio \cite{ben}, that the necessary condition for the system to robustly store discrete state information for the long-term results in a sufficient condition of gradient decay. The issue of vanishing gradient \cite{hoch} or exploding thereof when training RNNs, affecting long-range temporal dependencies, hinders the ideal application. 

Launching RNNs has thus been slowed as despite the straightforward concept introduced in 1986 \cite{RNN1}, implementing has been fraught with difficulties in capturing long-range temporal dependencies and slow computation. With advances in hardware, these problems have been somewhat mitigated, and RNNs exist in modern applications. However, the prospect of keeping RNNs small or NNs, in general, is engaging.

The vanishing gradient phenomenon can be exemplified when utilizing the conventional training algorithm backpropagation through time (BPTT), first unfolding the network, then backpropagating the error through the network \cite{DRNNS}. The connections between nodes or weights propagate the error. Small changes initially permeate the network, possibly causing exponential diminishing or increase of individual later weights in the network, affecting the gradient \cite{field}. Additionally, with this naïve algorithm, unfolding and effectively producing a feed-forward network, training would be practically impossible for a large number of iterations of repetitions through the network. An attempt at solving this is by truncating and not unfolding the whole network at every step, possibly using parallelization to calculate in parts. However, this would consequentially truncate long-term dependencies simultaneously. Seemingly a Mexican stalemate, efforts to remedy this have been somewhat successful. 

One early method of mitigating the effects came with so-called NARX networks (Nonlinear AutoRegressive with eXogenous inputs method) \cite{DRNNS}, which introduce direct connections between current and past hidden states, reducing nonlinearities inherent in the RNN structure. However, these architectures only alleviate by a constant factor the duration of temporal dependencies that can be learned \cite{suts}.

A stochastic approach termed the echo state network (ESN) employs random connections but suffers from sizeable structural overhead and is unable to cope with data-intensive problems. The method stems from the more general approach of random weight guessing (RG), which suffers in complex structures only being reliable under solution-dense conditions. However, RG could be used in benchmark evaluation for identifying faulty design. Moreover, the random structure's impressive performance in specific domains suggests that it is worth investigating ESN-based initialization \cite{suts} or expanded random features. 

Leading RNN architecture is the long short-term memory \cite{LSTM} (LSTM), which is currently in use in state-of-the-art speech recognition and smart assistants \cite{Apple}. The LSTM uses memory units, linear and adept at controlling information to and from the unit, productively truncating the gradient without undermining performance. There has been further development and improvement on the LSTM structure, and several variants have been devised, tests have been conducted, and overall comparable performance was confirmed\cite{Greff}. The Gated recurrent units (GRUs) akin to the LSTM but with fewer parameters, are commonly applied alterations but has been shown to be inferior to the LSTM in application. However, complex models require more training. As has been established, LSTM has proceeded to solve pathological long-range temporal dependencies and was successfully applied to speech and handwritten text recognition \cite{gs, gas}, and robotic control \cite{mayer}. Attempts have been made to progress further by putting LSTMs in a grid\cite{ka} or, in combination with generative models and evaluation seems promising\cite{gr, ch, bo}. In conclusion, RNN and especially LSTM belong to the principally most general and powerful sequence processors out there.

LSTM has clear advantages over conventional RNNs; however, with the coming of Hessian-free optimization (HF) in addition to structural dampening, this edge is lessened. With this method, RNNs proceed to solve pathological long-term dependencies; computationally, it still lacks in comparison \cite{suts}. 

A problem with RNNs is the mapping of long to short sequences, which has been recently treated with attention-based models \cite{xu}. Earlier, the solution consisted of compressing the input, meanwhile losing information and spending resources compressing. Attention-based models brought the implementation of variable length memory, which gives flexibility in generating the output and accessing different parts of the memory at different output times. 

The active field of capturing long-time dependencies has been a focus - especially for RNNs - and in different ways avoiding the structure altogether seems to be the most recent advancements. However, the application of an in part pruning algorithm on an RNN is of interest as this could point to the versatility and benefit of such algorithms.

Besides the ones mentioned, challenges ahead include faster learning, representational capacity, and weights as states \cite{suts}. Faster learning relates to the high cost of calculating the gradient on long sequences; methods to approximate derivatives under such circumstances quickly and efficiently are crucial. The two latter pertain to the hidden state's size, the information, and computation related to an input sequence; by increasing the size, it is possible to achieve longer-term temporal dependencies. The hidden state's size is where the proposed algorithm's stochastic nature, changing the structure through time, aims to increase the hidden state's size and representational capacity.

Within deep learning, the other competing model is the convolutional neural network (CNN). This class is most commonly applied to image recognition or analyzing visual imagery in general. However, as of late, a variation has been designed to compete with RNNs on processing sequences, the so-called temporal CNN. Recent studies have been made showing the superiority of said architecture over RNNs\cite{tcnvsrnn}. However, the method presented is possibly applicable to CNNs too.

\section{Regularization}

Apart from these challenges, for RNNs, a more general problem for any statistical model is overfitting. This phenomenon spawns when a neural network (NN) model includes more adjustable parameters than necessary, or an overly complicated way of achieving fitness is used. Specifically, overfitting violates Occam's razor in which any given complex function is a priori less probable than any given simple function. Battling overfitting regularization schemes are introduced. Since overfitting is increasingly likely to happen by prolonging the training scheme, the necessity for regularization increases.

Several forms of regularization exist. A term is commonly added to the optimization problem, which penalizes the sought function's complexity. A typical example is the Tikhonov regularization, which adds the L2-norm of the supposed solution and thus preferring smaller norms. The motivation for regularization is improving the generalization of the learned model; in this case, smaller norms equal better generalization. Other regularization examples include early stopping, which incorporates a statistically independent validation set on which performance is measured, and training continues until performance no longer improves. Pruning is another method in which one targets connections or parameters of the NN itself and removes them, producing sparse networks but this entails retraining the pruned network \cite{Yann}. The removal is decided by calculating either the sensitivity of removing weights on the cost function or only the magnitude of each weight, claiming that relatively small magnitudes have small effects on the cost function. Additionally, a validation set can be used to cross-validate the cost function improvement on a separate set \cite{prunings}. 

The introduced algorithm is a regularization scheme that varies the structure to improve performance and simultaneously reduce the NN complexity. Firstly structural updates or changes are defined to limit how a NN can be changed. Starting from a fully connected network and changing the structure throughout the training. The proposed updates are deleting a node, duplicating a node, and deleting a weight or edge. The NN is first trained on the training part of the data set, called the training set. The NNs structure is then perturbed in the ways described above, measuring energy before and after each separate update is made. The updates are benchmarked on an external data set, distinct from the training set called selection or validation set, to promote generalization. Based on the energy differences in the cost function of each of said update, a softmax decision scheme is used to determine the update made. 

The thesis will consider a proof-of-concept of the introduced algorithm and regularization, using HF optimization for the RNN. Additionally, the algorithm will treat the standard data set MNIST, Modified National Institute of Standards and Technology\cite{mnist}. A possible novel application is the regularization of other neural networks by the suggested method.

\section{Motivation}

Whenever an NN is produced, trained, and benchmarked, there is a so-called "Brasklapp" that albeit the NN or NN-related algorithm performs well, there is a possibility that the hyperparameters regarding structure or other were not optimal. However, this is not a well-studied area. The implications of optimizing structure are a possible performance boost or faster training if a smaller NN could produce similar results. A pruning strategy is often implemented, effectively reducing the size and simultaneously the complexity of the NN.  

The task of hyperparameter optimization or restructuring is daunting and fraught with nonlinearities. In one way, it has been handled by generating a set of architectures, such as incrementally increasing the hidden layer and letting them go through the training procedure in parallel. This brute force method of testing some set of reasonable structures post-training completely avoids the problem and relies heavily on hardware; it is not a scalable method as the NN size increases, more computational power is required to in parallel train the NNs. The generated architectures are then benchmarked and cross-validated on a different set called selection set, which decides the then used NN. 

This thesis's effort is to streamline the procedure of perturbing architectures into an MC-walk; the randomness of the algorithm is introduced as there is no a priori optimal way to change the network. Regardless of the sophisticated method, in the end, what is strived for is a generalization. Firstly a set of updates or changes are decided. The updates are inspired by an organic NN where cells can divide or self-destruct, and their connections can change throughout the NN's lifetime. 

The induced changes are then thought of as fitness attributes, as in evolutionary biology, or a perturbance in which the most significant generated advantage should prevail. However, all data involved in the process only represent to a certain degree of the real test data set. Thus, selecting the best performing update is not guaranteed to generate, in the end, the best model. Ergo random factors decide which update to make; after more study, a preference could be established, such as always pruning the network. The duplication is included as even though the overall complexity might increase, on one level, redundancy is introduced and, as such, should not vastly change the intricacy. 

Similarly, the process could be thought of as a stochastic process proceeding towards equilibrium. In physics, a statistical ensemble is an idealization of a physical system relating to the probability distribution over possible physical states. If a system is at fixed volume and temperature, but with variable energy and number of particles, it is called a grand-canonical ensemble. Typically when simulating a grand-canonical ensemble, grand-canonical Monte Carlo is used. As is often the case in physics sampling, macroscopic quantities averages converge until only statistical fluctuations remain, analogously treating changes in the NN energy function of a mini-batch as such towards the end of the training. Additionally, particles, in this case, nodes or neurons, can be inserted or removed, and edges can be removed. The NN changes are sent through a similar scheme as in the simulation of the physical system; however, more consideration is taken when generating or deleting nodes as no governing distribution exists for NNs. This is due to the fact that distributions concerning NN parameters are a problem adapted, akin to a configurational bias implementation of the Monte Carlo method. 

\section{Related Work}

The idea of modifying the structure of the neural network in time is not unfamiliar, and a related significant breakthrough is the dropout technique \cite{drop}. Both algorithms can be seen as adding noise to hidden units; the proposed stochastic structure (MCR) takes this one step further by allowing mutation. Formally a stochastic regularization technique, dropout attempts to increase generalization performance by reducing overfitting during learning. Similar to MCR, dropout instead tackles the challenge of overfitting by dropping units from the network, making the network more straightforward and thus easier to train. Additionally, this opens the possibility of combining exponentially many different network architectures efficiently, drawing from ensemble learning; this constitutes a pseudo-ensemble. However, dropout cannot be applied straightforwardly to RNNs \cite{dropno} but has to be limited to a certain subset\cite{droprnn} to be fruitful. The paper by \textcite{droprnn} only treats the LSTM structure and claim that it could be successfully applied to any RNN. 

Several other papers treat overfitting by applying dropout to the connections in varying ways, limiting to the feed-forward network, only the non-recurrent connections, or the hidden states update vector. The further modification includes the so-called Fracternal and Curriculum dropout or the stochastic depth; the last one consists of dropping an entire layer\cite{stochdep}. By combining the dropout method and DropConnect \cite{DropConnect}, a generalization of dropout for large fully-connected layers within structures and other methods resulted in a successful variant of the LSTM, namely the averaged stochastic gradient descent (ASGD) Weight-Dropped LSTM, AWD-LSTM for short. A landmark neural network architecture in language-modeling. 

Another approach is the so-called Zoneout\cite{zoneo}, where instead of dropout, a zoneout would revert the unit's activations to the previous timestep — injecting noise into the network, a mask designating which units are to be affected. Zoneout has the benefit of keeping propagation of gradients efficient and preserving state information, akin to feed-forward stochastic depth networks\footnote{Not to be confused with stochastic feed-forward neural networks. Despite their alluding name, stochastic feed-forward networks are related to Restricted Boltzmann Machines, which learn probability distributions over its set of inputs.}. By optimally designating the occurrence of zoneout, state of the art advancements have been made\cite{Zoneout}. In the article by \textcite{Zoneout}, a concept surprise is introduced, the discrepancy between the last state's prediction and target. Surprisal could further be an optimum way of determining the stochastic evolution of MCR. The authors \textcite{Zoneout} conjecture that stochasticity leads to robustness in the hidden state. As mentioned, MCR is designed to increase the hidden state, and surprise implementation will be left out.

Weakening the stochastic activation of dropout in the sense of periodically managing information flow through the network is another method. One such model is the Phased LSTM \cite{phaselstm}, which specializes in asynchronous sensory events that carry timing information. Another is the Clockwork RNN \cite{Clockwork}, a NN architecture which partitions the hidden layer into distinct modules of temporal granularity, performing computations at its imposed clock rate. The proposed model takes one step into generalizing by delving into the stochastic forming of the network itself.

The attention-based model mentioned earlier was first introduced by \textcite{att}, improving the encoder decoder-based approach, which utilizes RNNs typically for machine translation. By weighing information based on a context given by the encoder, then generating a sentence from a picture or translating a sentence with the decoder, usually an LSTM. As RNNs and LSTMs still suffer from being unable to capture long data sequences, attention-based models are an effort to counter the problem by screening information. The concept has evolved into altogether attention-based architectures without RNNs such as the Transformer \cite{trans}, which has outperformed its predecessors. This introduced the notion of self-attention. The MCR could be thought of in a similar manner, where self-reflection is made to evaluate the current structure. 

Of general importance is the universal approximation theorem, which states that a feed-forward neural network is able to approximate any Borel measurable function on a compact domain. A proof has been done even for the case of RNN \cite{uarnn} and of interest is the particular evolving structure and its implications of the proposed MCR. However, this will not be studied in this thesis.
