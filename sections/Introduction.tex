
\chapter{Introduction}

In this master thesis the prospect of a recurrent neural network, RNN, with a stochastic structure varying in time to regularize it, Monte Carlo RNN, MCRNN, is presented. Inspiration is drawn from biology where neural networks organically evolve their structure, formalized in the concept of a Markov process. How this algorithm is structured, however, is inspired from physics processes where energy differences drive the algorithm. 

RNNs are a class derived from feedforward neural networks with iteration over a fixed structure to achieve temporal dynamics \cite{DRNNS}. Forming along the time axis is a directed graph, achieving dynamic behavior in the sense of memory. Utilizing the memory to capture patterns also requires dynamical programming for the necessity of mapping long sequences to short. The RNNs excel at pattern recognition in data sequences e.g. natural language processing, machine translation and most commonly speech recognition \cite{handwriting}. 

Initially RNNs were called dynamic recurrent neural networks, DRNs, displaying time-varying responses as per dynamical systems. During the early stages of development, a key issue was, shown by Bengio \cite{ben}, that the necessary condition for the system to robustly store discrete state information for the long-term results in a sufficient condition of gradient decay. The issue of vanishing gradient \cite{hoch} or exploding thereof when training RNNs, affecting long-range temporal dependencies hinders the ideal application. 

Launching RNNs has thus been slowed as despite the straightforward concept introduced in 1986 \cite{RNN1}, implementing has been fraught with difficulties of capturing long-range temporal dependencies and of slow computation. With advances in hardware this problems have been somewhat mitigated and RNNs exist in modern applications.

The phenomenon of vanishing gradient can be exemplified when utilizing the conventional training algorithm backpropagation through time, BPTT, first unfolding the network then backpropagating the error through the network \cite{DRNNS}. Small changes permeate the network possibly causing exponential diminishing or increase of certain later weights in the network, affecting the gradient \cite{field}. Additionally, with this na√Øve algorithm, unfolding and effectively producing a feed forward network, training would be impossible for a large number of iterations. An attempt at solving this is by truncating and not unfolding the whole network at every step, possibly using parallelization to calculate in parts. However, this would consequentially truncate long-term dependencies simultaneously. Seemingly a Mexican stalemate, efforts to remedy this have been somewhat successful. 

One early method of mitigating the effects came with so-called NARX networks (Nonlinear AutoRegressive with eXogenous inputs method) \cite{DRNNS} which introduce direct connections between current and past hidden states, reducing nonlinearities inherent in the RNN structure. But these architectures only alleviate by a constant factor the duration of temporal dependencies that can be learned \cite{suts}.

A stochastic approach termed the echo state network, ESN, employs random connections but suffers from large structural overhead and is unable to cope with data-intensive problems. This stems from the more general approach of random weight guessing, RG, which suffers in complex structures only being reliable under solution-dense conditions. This could be used in benchmark evaluation for identifying faulty design. Moreover, the random structures impressive performance in certain domains suggests that it is worth investigating ESN-based initialization \cite{suts}. 

Leading RNN architecture is the long short-term memory \cite{LSTM}, LSTM, which is currently in use in state-of-the-art speech recognition and smart assistants \cite{Apple}. The LSTM use memory units, linear and adept at controlling information to and from the unit, productively truncating the gradient without undermining performance. There has been further development and improvement on the LSTM structure and several variants have been devised, tests have been conducted and an overall comparable performance was confirmed\cite{Greff}. The Gated recurrent units, GRUs, akin to the LSTM but with fewer parameters is one commonly applied alteration but has been shown to be lesser than the LSTM in application. However, complex models require more training. As has been established LSTM has proceeded to solve pathological long-range temporal dependencies and was successfully applied to speech and handwritten text recognition \cite{gs, gas}, robotic control \cite{mayer}. Attempts have been made to progress further, putting LSTMs in a grid\cite{ka} or in combination with generative models and evaluation seems promising\cite{gr, ch, bo}. In conclusion, RNN and especially LSTM belong to the principally most general and powerful sequence processors out there.

LSTM has clear advantages over conventional RNNs, however, with the coming of Hessian free optimization, HF, in addition to structural dampening this edge is lessened. With the method RNNs proceed to solve pathological long-term dependencies, but computationally it still lacks in comparison \cite{suts}. 

From the RNN formulation above a problem with RNNs is the mapping of long to short sequences and this has been recently treated with attention-based models \cite{xu}. Earlier the solution consisted in compressing the input meanwhile losing information and spending resources compressing. With attention-based models came the implementation of variable length memory which gives flexibility in generating the output as well as accessing different parts of the memory at different output times. 

Challenges ahead, besides the ones mentioned, include faster learning, representational capacity and weights as states \cite{suts}. Faster learning relates to the high cost of calculating the gradient on long sequences, methods to approximate derivatives under such circumstances quickly and efficiently are crucial. The two latter pertain to the size of the hidden state, the information and computation related to a sequence, by increasing the size it is possible to achieve longer-term temporal dependencies. This is where the stochastic nature of the proposed algorithm, changing the structure through time, aims to increases the size of the hidden state.

The thesis will consider a proof of concept of the introduced algorithm and regularization, using HF optimization for the RNN. Additionally, the algorithm will treat the standard data set MNIST, Modified National Institute of Standards and Technology\cite{mnist}. A possible novel application is the regularization of other neural networks when prompted with the suggested method.

Within deep learning the other competing model is the convolutional neural network, CNN. This class is most commonly applied to image recognition or analyzing visual imagery in general. However, as of late, a variation has been designed to compete with RNNs on processing sequences, the so called temporal CNN. Recent studies have been made showing superiority of said architecture over RNNs\cite{tcnvsrnn},however, the method is possibly applicable to CNNs too.

\section{Motivation}

Whenever a neural network, NN, is produced and benchmarked there is a so called "Brasklapp" that albeit the NN or NN related algorithm performing well there is a possibility that the hyperparameters regarding structure or other was not optimal. However, this is not well studied area. The implications of optimizing structure are a possible performance boost or faster training if a smaller NN could produce similar results. 

The task of hyperparameter optimization is daunting and fraught with nonlinearities and has been handled in one way by generating a set of architectures, such as incrementally increasing the hidden layer, and in parallel letting them go through the training procedure. This brute force method of testing all reasonable structures after training completely avoids the problem and relies heavily on hardware, it is not a scalable method as the size of the NN increases more computational power is required to in parallel train the NNs. The generated architectures are then benchmarked on a different set called selection which decides the then used NN. 

The effort here is to streamline the procedure of perturbing architectures into a MC-walk, the randomness of the algorithm is introduced as there is no a priori optimal way to change the network. 

\section{Related Work}

The idea of modifying the structure of the neural network in time is not unfamiliar and a related major breakthrough is the dropout technique \cite{drop}. Both algorithms can be seen as adding noise to hidden units, the proposed stochastic structure, MCRNN, takes this one step further by allowing mutation. Formally a regularization technique, dropout is an attempt to increase generalization performance by reducing overfitting during learning. Similar to MCRNN dropout instead tackles the challenge of overfitting by dropping units or nodes from the network, making the network simpler and thus easier to train. Additionally, this opens for the possibility of combining exponentially many different networks architectures efficiently, drawing from ensemble learning this constitutes a pseudo-ensemble. However, dropout cannot be applied in a straightforward manner to RNNs \cite{dropno} but has to be limited to a certain subset\cite{droprnn} to be fruitful. The paper by \textcite{droprnn} only treats the LSTM structure and claim that it could be successfully applied to any RNN. 

Several other papers treat the issue by applying dropout to the connections in varying ways, limiting to the feedforward network, only the non-recurrent connections or the hidden states update vector. Further modification include the so-called Fracternal and Curriculum dropout or the stochastic depth, the last one consists of dropping an entire layer\cite{stochdep}. Combining the method of dropout and DropConnect \cite{DropConnect}, a generalization of dropout for large fully-connected layers within structures, and other methods resulted in a successful variant of the LSTM, namely the avereged stochastic gradient descent(ASGD) Weight-Dropped LSTM, AWD-LSTM for short. A landmark neural network architecture in language-modeling. 

Another approach is the so called Zoneout\cite{zoneo}, where instead of dropout a zoneout would revert the units activations to previous timestep. Injecting noise into the network, using a mask designating which units are to be affected. This has the benefit of keeping propagation of gradients efficient and preserving state information, akin to feedforward stochastic depth networks\footnote{Not to be confused with stochastic feedforward neural networks. Despite their alluding name, stochastic feedforward networks are more related to Restricted Boltzmann Machines, RBMs, which learn probability distributions over its set of inputs.}. By optimally designating occurence of zoneout, state of the art advancements have been made\cite{Zoneout}. In the article by \textcite{Zoneout} a concept surprise is introduced, the discrepancy between the last state's prediction and target. Surprisal could further be an optimum way of determining the stochastic evolution of MCRNN. The authors \textcite{Zoneout} conjecture that stochasticity leads to robustness in the hidden state. As mentioned, MCRNN is designed with a the intention of increasing the hidden state and implementation of surprise will be left out.

Weakening the stochastic activation of dropout in the sense of periodically managing information flow through the network is another method. One such model is the Phased LSTM \cite{phaselstm} which specializes in asynchronous sensory events that carry timing information. Another is the Clockwork RNN \cite{Clockwork}, a NN architecture which partitions the hidden layer into distinct modules of temporal granularity, performing computations at its imposed clock rate. The proposed model takes one step into generalizing by delving into stochastic forming of the network itself.

Mentioned earlier was the attention-based model, first introduced by \textcite{att} improving on the encoder decoder-based approach which utilizes RNNs typically for machine translation. As RNNs and LSTMs still suffer from being unable to capture long data sequences, attention-based models are an effort to counter the problem by screening information. Weighing information based on a context given by the encoder, generating a sentence from a picture or translating a sentence with the decoder, usually an LSTM. The concept has evolved into altogether attention-based architectures without RNNs such as the Transformer\cite{trans} which has outperformed its predecessors. This introduced the notion of self-attention.

Of general importance is the universal approximation theorem, which states that a feedforward neural network is able to approximate any Borel measurable function on a compact domain. A proof has been done even for the case of RNN \cite{uarnn} and of interest is the particular evolving structure and its implications of the proposed MCRNN. However, this will not be studied in this thesis.

%attention, resnet, self-attention, tcm, hidden state theory, physics context