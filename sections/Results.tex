\chapter{Results}

The results gathered are from a randomly initialized RNN on the data set MNIST. The MNIST data set consists of $28 \times 28$ pixel handwritten digit images. The task is to classify the images into $10$ digit classes. For the RNN, the image is sequentially fed into the RNN, here $28$ pixels at a time. 

The input and output dimensions decide the architecture of the RNN. The input layer then consists of $28$ nodes, the output $10$. The hidden layer size is variable and initial sizes are set. The weights are randomly initialized for the symmetry break of the RNN learning, and biases are left at zero. 

The MNIST data set is divided into mini-batches. A mini-batch consists of $1000$ samples, as the HF relies on bigger mini-batches. An epoch is a complete progression of mini-batches through the entire data set, of which there are in total $60 000$. However, of the entire set, a selection set of $10 000$ samples was used, making the training set $50000$. A total of $11$ epochs was performed on one training run. Finally, a separate test set of $10000$ samples benchmarked the RNN. The initialization is omitted from the results, consisting of the first epoch, $50$ mini-batches. The omittance is to resolve the more critical training part better.

The activation for the hidden layer $a_h(x) = 1/(1 + e^{-x})$ is the sigmoid. The output or output activation \[a_o(z_j) = \frac{e^{z_j}}{\sum_{k=1}^N e^{z_k}}\] is the softmax function. Accordingly the "matching" cost function is the cross-entropy function \[C(z,x) = - \sum_i [x_i] \log [\hat{x}]_i\] where $\hat{x} = a_o(z)$ is the models prediction and $x$ is the target. Using one-hot encoding the target $x$ is a vector of zeroes except at one location. The "matching" property leads to \[\nabla_z C(z,x) = \hat{x} - x \text{ and } H_{C(z,x)} = \text{diag}(\hat{x}) - \hat{x} \hat{x}^\top\] which is left as an exercise to the reader but what can be seen is a benign Hessian independent of the target. 

For the changes in the RNN, a relaxation function $exp(5/(2n)) - 1$ where $n$ is the current epoch, was used to regulate changes and make them less probable as the training progresses. To this a uniform random number $r$ is drawn and if smaller $ r < exp(5/(2n)) - 1$ the perturbation will proceed.

\section{Setups and Figures}

Different initial sizes of the hidden layer were studied, comparing with and without the regularization for each size. The sizes were $L_h = 27, 37, 47, 57, 67$, from which the training started. However, the RNNs sizes with regularization changed throughout the training. How this happened is ignored as the moves are incomparable; removing a weight vs. a node is an altogether different change, and all the moves are treated without bias in the algorithm. 

In the figures, w in the legends stands for with MCR and the hidden layer initial size (HLS). 

\begin{figure}[h]
    \includegraphics[width = \textwidth]{imgs/ps.pdf}
    \caption{Cross-entropy plot on the mini-batch for various initial sizes of the RNN, omitting first epoch. With and without MCR, w in the legends stands for with MCR.}
    \label[fig]{E}
\end{figure}

\begin{figure}[h]
    \includegraphics[width = \textwidth]{imgs/ps1.pdf}
    \caption{Cross-entropy plot on the selection set for various initial sizes of the RNN, omitting first epoch. With and without MCR, w in the legends stands for with MCR.}
    \label[fig]{E_sel}
\end{figure}

As can be seen, a figure of the cross-entropy energy $E$ during training. Fig. \ref{E} of the energy on the current mini-batch after the corresponding update and Fig. \ref{E_sel} of the energy on the selection set before the update. The variation is due to the mini-batch being a small subset, and thus an error is introduced, and minimizing on the rest of the data set is not guaranteed. As can be seen, somewhat is the energy of hidden size 67 close to the bottom in Fig. \ref{E}, \ref{E_sel}. The difference in overall decrease is due to the selection set being fixed; meanwhile, a mini-batch is variable. However, these plots are dense in data, and thus averages and standard deviations are also included. 

\begin{figure}[h]
    \includegraphics[width = \textwidth]{imgs/ps2.pdf}
    \caption{Average cross-entropy across training, after the first four epochs. For the mini-batch and sel, selection set. With and without MCR, w in the legends stands for with MCR.}
    \label[fig]{avg}
\end{figure}

\begin{figure}[h]
    \includegraphics[width = \textwidth]{imgs/ps3.pdf}
    \caption{Standard deviation of the cross entropy across training, after the first four epochs. For the mini-batch and sel, selection set. With and without MCR, w in the legends stands for with MCR.}
    \label[fig]{std}
\end{figure}

In the plots of the average in Fig. \ref{avg} and standard deviation in Fig. \ref{std}, in the average, a clear difference with or without MCR can be seen, which was stable over epochs. Especially for medium-sized RNNs. However, the standard deviation varied greatly depending on the number of epochs included in the calculation; this is due to randomly generated mini-batches.  

\begin{figure}[h]
    \includegraphics[width = \textwidth]{imgs/ps4.pdf}
    \caption{Error on test set, with and without MCR where w in the legends stands for with MCR.}
    \label[fig]{acc}
\end{figure}

The last plot of the error on the test set Fig. \ref{acc} with and without MCR, which seems to perform well for small to medium-sized NNs, in this case, increasing performance and lowering error percentage.  

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
       Hidden Layer Size & 27 & 37 & 47 & 57 & 67 & \\
    \hline
       without & 6.82 & 11.98 & 7.66 & 5.54 & 3.74 & \% \\
    \hline
       with & 5.73 & 5.53 & 5.75 & 6.46 & 4.99 & \% \\
    \hline
    \end{tabular}
    \caption{Error in \%, with and without MCR.}
    \label[table]{acc_t}
\end{table}

In the Tbl. \ref{acc_t}, the error can be seen in plain text, with and without the regularization. Here an apparent increase is seen for small to medium-sized RNNs is seen with MCR.
