\chapter{Results}

The results gathered are from a randomly initialized RNN on the data set MNIST. The MNIST data set consists of $28 \times 28$ pixel handwritten digit images. The task is to classify the images into $10$ digit classes. A mini-batch consists of $1000$ samples, as the HF relies on bigger mini-batches. An epoch is a complete progression of mini-batches through the entire data set, of which there are in total $60 000$. Of the total set, however, a selection set of $10 000$ samples was used, which makes the training set $50000$. A total of $11$ epochs was performed on one training run. Finally a separate test set of $10000$ samples benchmarked the RNN. The initialization is omitted which consists of the first epoch, $50$ mini-batches.

The activation for the hidden layer $a_h(x) = 1/(1 + e^{-x})$ is the sigmoid. The output or output activation \[a_o(x_j) = \frac{e^{x_j}}{\sum_{k=1}^N e^{x_k}}\] is the softmax function. Accordingly the "matching" cost or energy function is the cross-entropy function \[- \sum_i [x_i] \log [x]_i\]. For the changes in the RNN, a relaxation function $e(5/(2x)) - 1$ where $x$ is the current epoch, was used to regulate changes and make them less probable as the training progresses.

\section{Size}

\section{}