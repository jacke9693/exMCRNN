\chapter{Results}

The results gathered are from a randomly initialized RNN on the data set MNIST. The MNIST data set consists of $28 \times 28$ pixel handwritten digit images. The task is to classify the images into $10$ digit classes. The data set is divided into mini-batches, a mini-batch consists of $1000$ samples, as the HF relies on bigger mini-batches. An epoch is a complete progression of mini-batches through the entire data set, of which there are in total $60 000$. Of the total set, however, a selection set of $10 000$ samples was used, which makes the training set $50000$. A total of $11$ epochs was performed on one training run. Finally a separate test set of $10000$ samples benchmarked the RNN. The initialization is omitted which consists of the first epoch, $50$ mini-batches.

The activation for the hidden layer $a_h(x) = 1/(1 + e^{-x})$ is the sigmoid. The output or output activation \[a_o(z_j) = \frac{e^{z_j}}{\sum_{k=1}^N e^{z_k}}\] is the softmax function. Accordingly the "matching" cost or energy function is the cross-entropy function \[C(z,x) = - \sum_i [x_i] \log [\hat{x}]_i\] where $\hat{x} = a_o(z)$ is the models prediction and $x$ is the target. Using one-hot encoding the target $x$ is a vector of zeroes except at one location. The "matching" property leads to \[\nabla_z C(z,x) = \hat{x} - x \text{ and } H_{C(z,x)} = \text{diag}(\hat{x}) - \hat{x} \hat{x}^\top\] which is left as an exercise but what can be seen is a benign Hessian independent of the target. 

For the changes in the RNN, a relaxation function $e(5/(2n)) - 1$ where $n$ is the current epoch, was used to regulate changes and make them less probable as the training progresses.

\section{Size}

\section{}