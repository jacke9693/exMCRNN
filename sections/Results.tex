\chapter{Results}

The results gathered are from a randomly initialized RNN on the data set MNIST. The MNIST data set consists of $28 \times 28$ pixel handwritten digit images. The task is to classify the images into $10$ digit classes. For the RNN the image is sequentially feeded into the RNN, here $28$ pixels at a time. 

The MNIST data set is divided into mini-batches, a mini-batch consists of $1000$ samples, as the HF relies on bigger mini-batches. An epoch is a complete progression of mini-batches through the entire data set, of which there are in total $60 000$. Of the total set, however, a selection set of $10 000$ samples was used, which makes the training set $50000$. A total of $11$ epochs was performed on one training run. Finally a separate test set of $10000$ samples benchmarked the RNN. The initialization is omitted from the results which consists of the first epoch, $50$ mini-batches.

The activation for the hidden layer $a_h(x) = 1/(1 + e^{-x})$ is the sigmoid. The output or output activation \[a_o(z_j) = \frac{e^{z_j}}{\sum_{k=1}^N e^{z_k}}\] is the softmax function. Accordingly the "matching" cost or energy function is the cross-entropy function \[C(z,x) = - \sum_i [x_i] \log [\hat{x}]_i\] where $\hat{x} = a_o(z)$ is the models prediction and $x$ is the target. Using one-hot encoding the target $x$ is a vector of zeroes except at one location. The "matching" property leads to \[\nabla_z C(z,x) = \hat{x} - x \text{ and } H_{C(z,x)} = \text{diag}(\hat{x}) - \hat{x} \hat{x}^\top\] which is left as an exercise but what can be seen is a benign Hessian independent of the target. 

For the changes in the RNN, a relaxation function $e(5/(2n)) - 1$ where $n$ is the current epoch, was used to regulate changes and make them less probable as the training progresses.

\section{Sizes and Plots}

Different initial sizes were studied, comparing with and without the regularization for each size. The sizes were $27, 37, 47, 57, 67$, from which the training started, however, the RNNs sizes with regularization changed throughout the training. The way in which this happened is ignored as the moves are incomparable, removing a weight vs a node is an altogether different change and all the moves are treated without bias in the algorithm. 

In the below plots, w in the legends stands for with the algorithm. 

\begin{figure}[h]
    \includegraphics[width = \textwidth]{imgs/ps.pdf}
    \caption{Cross-entropy plot on the mini-batch for various initial sizes of the RNN, omitting first epoch.}
    \label[fig]{E}
\end{figure}

\begin{figure}[h]
    \includegraphics[width = \textwidth]{imgs/ps1.pdf}
    \caption{Cross-entropy plot on the selection set for various initial sizes of the RNN, omitting first epoch.}
    \label[fig]{E_sel}
\end{figure}

As can be seen, a plot of energy during training. \ref{E} on the current mini-batch and \ref{E_sel} on the selection set. The variation is due to the mini-batch being a small subset.

\begin{figure}[h]
    \includegraphics[width = \textwidth]{imgs/ps2.pdf}
    \caption{Average cross-entropy across training, after the first four epochs. For the mini-batch and sel, selection set.}
    \label[fig]{avg}
\end{figure}

\begin{figure}[h]
    \includegraphics[width = \textwidth]{imgs/ps3.pdf}
    \caption{Standard deviation of the cross entropy across training, after the first four epochs. For the mini-batch and sel, selection set.}
    \label[fig]{std}
\end{figure}

In the plots of average \ref{avg} and standard deviation \ref{std}, a clear difference with or without the algorithm in the average which was stable. The standard deviation, however, varied greatly depending on the amount of epochs included in the calculation, this is due to randomly generated mini-batches.  

\begin{figure}[h]
    \includegraphics[width = \textwidth]{imgs/ps4.pdf}
    \caption{Accuracy on test set.}
    \label[fig]{acc}
\end{figure}

In the last plot of the accuracy on the test set \ref{acc} with and without the algorithm, which seems to perform well for small to medium sized NNs.
