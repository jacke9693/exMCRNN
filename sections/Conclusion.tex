\chapter{Conclusions}

Monte Carlo Regularization is a method for reducing overfitting in neural network training. This thesis's purpose was the proof-of-concept of this regularization, which targets the network's nodes or connections and removes or duplicates them. In doing so, pathological redundance could be eliminated and beneficial inserted into the network. The effects of the method have been established, a significant improvement was found on the MNIST for small to medium-sized recurrent neural networks compared to recurrent neural networks of similar size without the method. Further study is required to establish a scaling in the activity of the regularization for larger NNs. The method is general and could be applied to arbitrary kind of neural network, the reason for using recurrent neural networks is that their non-linear structure complicate the benchmarking of structural perturbations.

The study and measurement of the hidden state size was omitted; however, a very rudimentary comment is that allowing for an increase in the hidden layer size directly increases the hidden state's size. Not included was the activity or selection of parameter of the method which should be properly studied. The neccesary scaling could be established accordingly. 

The regularization lengthened the training; however, comparatively, the Hessian-free optimization approach was the dominant part of the elapsed time. However, utilizing stochastic gradient descent could reduce the relative times, and optimizing the regularization could then be of importance. Speeding up the process overall, thereby not relying on a supercomputer, is a piece of advice for future study. Also, relying on existing libraries and open-source platforms for machine learning is a protip, not to reinvent the wheel in C++ but rather to depend on the flexibility of giants.

\section{Future work}

The algorithm hints at a method of initializing a network by continuously perturbing the network pre-training until some threshold value on a cross-validation set is met. 

A way to instead combat the standard deviation instability is regularizing the parameter update produced in the conjugate gradient method by perturbing and benchmarking it in a similar way to Monte Carlo Regularization on a selection set, inspired from micro-canonical sampling, which treats energy as a conserved quantity, to limit the effect on the selection set cross-entropy of a mini-batch update. The analogy is to a micro-canonical simulation. A demon considers changes in energy, for example, by producing an extensive update on the selection set and projecting smaller updates from mini-batches to update parameters by considering more information at each mini-batch or randomly benchmarking perturbing the generated updates.

Lastly, regulating the mini-batch size could be done similarly. If the selection set performance goes down to increase the mini-batch size and vice versa, somewhat like sampling different temperatures, regulating how much information is considered in each update of the NN parameters. Nevertheless, this relates to further study and merely is concepts derived from this successful regularization technique.