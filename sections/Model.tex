\chapter{Model Description}

This section describes firstly the RNN structure and secondly the regularization.

\section{Recurrent Neural Network}

The recurrent neural network, RNN, is a nonlinear dynamical system mapping sequences to sequences. RNNs are typically structured with one input $i$, one hidden $h$ and one output layer $o$. Being parametrized by three weight matrices and two bias vectors \([W_{hi}, W_{hh}, W_{oh}, b_h, b_o]\) the state concatenation denoted \(\theta\) completely describes the RNN. Additionally an initial hidden state can be included but $h_0$ the initial hidden state is set to the zero vector in this thesis.

The propagation through the network of an input sequence \(i_1, i_2, ... , i_T\) is performed in the following algorithm:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
        \State{$u_t \gets W_{hi}i_t + W_{hh}h_{t-1} + b_h$}
        \State{$h_t \gets a_h(u_t)$}
        \State{$o_t \gets W_{oh} + b_o $}
        \State{$z_t \gets a_o(o_t)$}
    \EndFor
\end{algorithmic}

Where $ a_h(*)$ and $a_o(*)$ are nonlinear activation functions for the hidden and output layer respectively. The cost function is usually defined as a sum of per-timestep cost:

\[C(z, y) = \sum_{t=1}^{T}C(z_t; y_t)\]

for some target sequence $y_t$. With the cost function a derivative for the RNNs can be computed by the backpropagation through time algorithm, BPTT:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
        \State{$do_t \gets a_o'(o_t) dC(z_t; y_t)/dz_t $}
        \State{$db_o \gets db_o + do_t$}
        \State{$dW_{oh} \gets dW_{oh} + db_oh_t^\top $}
        \State{$dh_t \gets dh_t + W_{oh}^\top do_t$}
        \State{$du_t \gets a_h'(u_t)dh_t $}
        \State{$dW_{hi} \gets dW_{hi} + du_ti_t$}
        \State{$db_h \gets db_h + du_t $}
        \State{$dW_{hh} \gets dW_{hh} + du_th_{t-1}^\top $}
        \State{$ dh_{t-1} \gets W_{hh}^\top du_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

As mentioned there are difficulties in training RNNs. Included in this thesis is the Hessian-Free, HF, optimization, which is a second order method but combined with the R-method the above algorithm is essentially sufficient. However, as stated in the work, the approximation called the Gaus-Newton matrix, \(G\), of the Hessian is preferred for stability. Described in the algorithm below is the product \(Gv\) for some vector $v$:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
        \State{$do_t \gets a_o'(o_t) dC(z_t; y_t)/dz_t $}
        \State{$db_o \gets db_o + do_t$}
        \State{$dW_{oh} \gets dW_{oh} + db_oh_t^\top $}
        \State{$dh_t \gets dh_t + W_{oh}^\top do_t$}
        \State{$du_t \gets a_h'(u_t)dh_t $}
        \State{$dW_{hi} \gets dW_{hi} + du_ti_t$}
        \State{$db_h \gets db_h + du_t $}
        \State{$dW_{hh} \gets dW_{hh} + du_th_{t-1}^\top $}
        \State{$ dh_{t-1} \gets W_{hh}^\top du_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}