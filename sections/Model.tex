\chapter{Model Description}

This section describes firstly the RNN structure and secondly the regularization.

\section{Recurrent Neural Network}

The recurrent neural network, RNN, is a nonlinear dynamical system mapping sequences to sequences. RNNs are typically structured with one input $i$, one hidden $h$ and one output layer $o$. Being parametrized by three weight matrices and two bias vectors \([W_{hi}, W_{hh}, W_{oh}, b_h, b_o]\) the state concatenation denoted \(\theta\) completely describes the RNN. Additionally an initial hidden state $h_0$ can be included but $h_0$ the initial hidden state is set to the zero vector in this thesis.

The forward propagation through the network of an input sequence \((i_1, i_2, ... , i_T)\) to an output sequence \((o_1, o_2, ..., o_T) \) is performed in the following algorithm:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$u_t \gets W_{hi}i_t + W_{hh}h_{t-1} + b_h$}
    \State{$h_t \gets a_h(u_t)$}
    \State{$z_t \gets W_{oh}h_t + b_o $}
    \State{$o_t \gets a_o(z_t)$}
    \EndFor
\end{algorithmic}

Where $ a_h(*)$ and $a_o(*)$ are nonlinear activation functions for the hidden and output layer respectively. The cost function is usually defined as a sum of per-timestep cost:

\[C(o, y) = \sum_{t=1}^{T}C(o_t; y_t)\]

for some target sequence $y_t$. With the cost function a derivative for the RNNs can be computed by the backpropagation through time algorithm, BPTT \cite{RNN1}:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$do_t \gets a_o'(o_t) dC(o_t; y_t)/do_t $}
    \State{$db_o \gets db_o + do_t$}
    \State{$dW_{oh} \gets dW_{oh} + db_oh_t^\top $}
    \State{$dh_t \gets dh_t + W_{oh}^\top do_t$}
    \State{$du_t \gets a_h'(u_t)dh_t $}
    \State{$dW_{hi} \gets dW_{hi} + du_ti_t$}
    \State{$db_h \gets db_h + du_t $}
    \State{$dW_{hh} \gets dW_{hh} + du_th_{t-1}^\top $}
    \State{$ dh_{t-1} \gets W_{hh}^\top du_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

As mentioned there are difficulties in training RNNs, resorting to second order methods, however, solves some issues. Included in this thesis is the Hessian-Free, HF, optimization, which is a second order method avoiding explicit calculation of the Hessian matrix. Combined with the R-method the two above algorithms are essentially sufficient for calculating the product $Hv$. The $R_v$ operator is defined as follows for $R_vx $ denoting the directional derivative of a $\theta$-dependent variable $x$ in the direction $v$:

\[R_vx = \lim_{\epsilon\to 0} \frac{x(\theta + \epsilon v) - x(\theta)}{\epsilon} = \frac{\partial x}{\partial\theta}v\]

As a derivative, the $R_v$ operator obeys the following rules of differentiation:

\begin{enumerate}
    \item $R_v(x+y) = R_vx + R_vy$
    \item $ R_v(xy) = (R_vx)y + (R_vy)x $
    \item $R_v(\gamma(x)) = (R_vx) J_{\gamma}(x)$
\end{enumerate}

where $J_\gamma(x)$ denotes the Jacobian of $\gamma$. As can be recognized these are linearity, the product rule and chain rule respectively. The $v$ will be henceforth suppressed and implied for compact notation, additionally the relation $R_vb = b^v$. The relation between the Hessian and R-operator $Hv = R(\nabla(\theta))$ establishes the algorithm for calculating the product in terms of the above algorithms, simply a forward followed by a backward pass. A rigorous treatment can be found in \cite{suts}.

However, as stated in the work \cite{suts}, the approximation called the Gaus-Newton matrix, \(G\), of the Hessian is preferred for stability. Described in the algorithm below is the product \(Gv\) for some vector $v$:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$Ru_t \gets R{W_{hi}}i_t + RW_{hh}h_{t-1} + W_{hh}Rh_{t-1} + Rb_h$}
    \State{$Rh_t \gets a_h'(u_t)Ru_t$}
    \State{$Rz_t \gets RW_{oh}h_t + W_{oh}Rh_t + Rb_o $}
    \State{$Ro_t \gets a_o'(o_t)Rz_t$}
    \EndFor
    \For{$t$ from $1$ to $T$}
    \State{$Rdo_t \gets a_o'(o_t) dC(z_t; y_t)/dz_t $}
    \State{$db_o \gets db_o + do_t$}
    \State{$dW_{oh} \gets dW_{oh} + db_oh_t^\top $}
    \State{$dh_t \gets dh_t + W_{oh}^\top do_t$}
    \State{$du_t \gets a_h'(u_t)dh_t $}
    \State{$dW_{hi} \gets dW_{hi} + du_ti_t$}
    \State{$db_h \gets db_h + du_t $}
    \State{$dW_{hh} \gets dW_{hh} + du_th_{t-1}^\top $}
    \State{$ dh_{t-1} \gets W_{hh}^\top du_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

The $Gv$ product can be achieved from $Hv$ by simply disregarding derivatives of none differential terms, treating them as constant.

For second order methods, including the HF approach, other measures need to be incorporated to guarantee stability and convergence. The main constitutent of HF is CG, the conjugate gradient method. For this a sub-objective based on a approximation of the objective $f(\theta)$ is minimized by the CG. The iterative minimization, specifically, given a parameter setting $\theta_n$ for a $\theta_{n+1}$ is found by partially optimizing \[q_{\theta_n} \equiv M_{\theta_n}(\theta) + \lambda R_{\theta_n}(\theta) \]. A simplified HF algorithm is stated below:

\begin{algorithm}
    \caption{HF - simplified}
    \begin{algorithmic}
        \State{$\delta_0 \gets 0$}
        \State{$ \lambda \gets 50$}
        \State{assign mini-batch for each n, $\{U_n\}$}
        \For{each mini-batch $n = 1,2,...$}
        \State{define $Bv = \frac{1}{\abs{U_n}}\sum_{(x,y)\in U_n} G_f((x,y);\theta_n)v + \lambda v$}
        \State{compute $b = -\frac{1}{\abs{U_n}}\sum_{(x,y)\in U_n} f'((x,y);\theta_n)$}
        \State{find $\delta_{n+1}$ with CG on $z^\top B z/2 + b^\top z $, $z_0 = \delta_{n-1}$}
        \State{adjust $\lambda$ according to Levenburg-Marquardt heuristic}
        \State{adjust $\delta_{n}\alpha$, where $\alpha$ is calculated with a back-tracking line search}
        \EndFor
    \end{algorithmic}
\end{algorithm}