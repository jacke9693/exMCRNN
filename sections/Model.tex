\chapter{Model Description}

This section firstly describes the underlying theory of computational graphs, secondly the RNN structure, and lastly, the regularization. A lot of this section is based on the work of \textcite{suts} and \textcite{Martens2012}.

\section{Differentiable Computational Graph}

Before introducing artificial neural networks as such, the mathematical foundation needs to be formalized as the regularization, to be well-defined, couples to the underlying theory of graphs. Firstly is the efficient computation of derivatives of any function $F(\theta)$ that can be evaluated with such a graph. 

Considering a graph over $N$ nodes, $1, ..., N$ where $I$ is the subset consisting of the input nodes and $N$ is the output node. Each node $i$ has a set of parents $A_i$, determining its input, and a differentiable function $f_i$. For the above an algorithm for evaluating $F(\theta)$ on a computational graph \cite{suts}:

\begin{algorithmic}[1]
    \State{Let $\theta$ be distributed between the input nodes $z_i, i \in I$}
    \For{$i=1$ to $N$ \textbf{if} $i\notin I$}
    \State{$x_i \gets \text{concat}_{j\in A_i}z_j$}
    \State{$z_i \gets f_i(x_i)$}
    \EndFor
    \State{\Return{$F(\theta) = z_N$}}
\end{algorithmic}

where every node $z_i$ can be vector-valued, the formalism relates straightforwardly to neural networks.

Introducing a training error of the form $C(F(\theta)) = C(z_N)$, $C$ is the cost function and $F(\theta)$ is the models prediction on all samples. The derivative of $C(z_N)$ w.r.t. $\theta$ is given by $F'(\theta)^\top C'(z_N)$. The backpropagation algorithm calculates the product $F'(\theta)^\top v$ for arbitrary vector $v$ with according dimensionality of $z_N$ \cite{suts}:

\begin{algorithmic}[1]
    \State{$dz_N \gets v$}
    \State{$dz_i \gets 0$ for $i<N$}
    \For{$i = N$ \textbf{to} $1$ \textbf{if} $i \notin I$}
    \State{$dx_i \gets f_i'(x_i)^\top dz_i$}
    \For{$j \in A_i $}
    \State{$dz_j \gets dz_j + \text{unconcat}^jdx_i$}
    \EndFor
    \EndFor
    \State{concatenate $dz_i$ for $i \in I$ onto $d\theta$}
    \State{\Return{$d\theta = F'(\theta)^\top v$ }}
\end{algorithmic}

$\text{unconcat}$ being the inverse of $\text{concat}$ such that: if $x_i = \text{concat}_{j\in A_i} z_j$ then $\text{unconcat}^j x_i = z_j$. 

This formalism allows for the easy and automated application of the R-operator \cite{pepe}. The $R_v$ operator is defined as follows for $R_vx $ denoting the directional derivative of a $\theta$-dependent variable $x$ in the direction $v$:

\[R_vx = \lim_{\epsilon\to 0} \frac{x(\theta + \epsilon v) - x(\theta)}{\epsilon} = \frac{\partial x}{\partial\theta}v\]

As a derivative, the $R_v$ operator obeys the following rules of differentiation:

\begin{enumerate}
    \item $R_v(x+y) = R_vx + R_vy$
    \item $ R_v(xy) = (R_vx)y + (R_vy)x $
    \item $R_v(\gamma(x)) = (R_vx) J_{\gamma}(x)$
\end{enumerate}

where $J_\gamma(x)$ denotes the Jacobian of $\gamma$. As can be recognized, these are linearity, the product rule, and chain rule, respectively. The $v$ will be henceforth suppressed and implied for compact notation; additionally, the relation $Rb = b^v$ establishes the parameter $b$ with respect to $v$.

Given the above, computation of $R(F(\theta))$ is described in the following algorithm \cite{suts}:

\begin{algorithmic}[1]
    \State{distribute input $v$ across input nodes $Rz_i$ for $i \in I$}
    \For{$i = 1$ \textbf{to} $N$ \textbf{if} $i \notin I$ do}
    \State{$Rx_i \gets \text{concat}_{j\in A_i} Rz_j$}
    \State{$Rz_i \gets f_i'(x_i) R x_i$}
    \EndFor
    \State{\Return{$Rz_N = F'(\theta) v $}}
\end{algorithmic}

The algorithms for differentiation can be proven with structural induction over the graphs and are omitted. The needs for this in the Hessian-Free approach becomes clear with the relation $Hv = R(\nabla f(\theta))$, where $H$ is the Hessian of $f$. 

From two graphs $G_1 = (V_1,E_1)$ and $G_2 = (V_2, E_2)$ a new graph can be generated. The operators, $D_i$ deletion of either node $v_i \in V_1$ or $D_{ij}$ deletion of weight $e_{ij} \in E_1$ and the copying $C_i$ of node $v_i \in V_1$, required for the moves or changes to the graph, relate to the binary operators of graphs in the following way:

\begin{enumerate}
    \item Copying $C_i$, if $v_i \in V_1$: $G_1 \cup G_2 / G_1 \cap G_2$ where $V_2 = \{v'_i\} \cup V_1$ and $E_2 = \{e'_{ij}, e'_{ji}\}$ $\forall j$ $\mid v_j \in V_1$
    \item Deletion $D_i $, if $v_i \in V_1$: $G_1 \cap G_2 = (V_1 \cap V_2, E_1 \cap E_2)$ where $V_2 = V_1 / v_i$ and $E_2 = E_1 / \{e_{ij}, e_{ji}\} $ $ \forall j \mid v_j \in V_1 $
    \item Deletion $D_{ij}$, if $e_{ij} \in E_1$: $G_1 \cap G_2 = (V_1 \cap V_2, E_1 \cap E_2)$ where $V_1 = V_2$ and $E_2 = E_1 / e_{ij}$
\end{enumerate}

The $'$ is defined to be a slight perturbation of the associated values in the computational graph such that $v'_i \notin V_1 \cap V_2$. And the union to be $G_1 \cup G_2 = (V_1 \cup V_2, E_1 \cup E_2)$.

\section{Recurrent Neural Network}

The recurrent neural network, RNN, is a nonlinear dynamical system mapping sequences to sequences. RNNs are typically structured with one input $i$, one hidden $h$, and one output layer $o$. Being parametrized by three weight matrices and two bias vectors \([W_{hi}, W_{hh}, W_{oh}, b_h, b_o]\) the state concatenation denoted \(\theta\) completely describes the RNN. Additionally, an initial hidden state $h_0$ can be included, but $h_0$ the initial hidden state is set to the zero vector in this thesis.

The forward propagation through the network of an input sequence \((i_1, i_2, ... , i_T)\) to an output sequence \((o_1, o_2, ..., o_T) \) is performed in the following algorithm \cite{suts}:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$u_t \gets W_{hi}i_t + W_{hh}h_{t-1} + b_h$}
    \State{$h_t \gets a_h(u_t)$}
    \State{$z_t \gets W_{oh}h_t + b_o $}
    \State{$o_t \gets a_o(z_t)$}
    \EndFor
\end{algorithmic}

where $h_0 = 0$. $a_h(*)$ and $a_o(*)$ are nonlinear activation functions for the hidden and output layer respectively. The cost function is usually defined as a sum of per-timestep cost:

\[C(o, y) = \sum_{t=1}^{T}C(o_t; y_t)\]

for some target sequence $y_t$. With the cost function a derivative for the RNNs can be computed by the backpropagation through time algorithm, BPTT \cite{RNN1}, and format from \cite{suts}:

\begin{algorithmic}[1]
    \For{$t$ from $T$ to $1$}
    \State{$dz_t \gets a_o'(z_t) d C(o_t; y_t)/d o_t $}
    \State{$dh_t \gets dh_t + W_{oh}^\top dz_t$}
    \State{$du_t \gets a_h'(u_t)dh_t $}
    \State{$db_o \gets db_o + dz_t$}
    \State{$db_h \gets db_h + du_t $}
    \State{$dW_{oh} \gets dW_{oh} + dz_th_t^\top $}
    \State{$dW_{hh} \gets dW_{hh} + du_th_{t-1}^\top $}
    \State{$dW_{hi} \gets dW_{hi} + du_ti_t^\top$}
    \State{$ dh_{t-1} \gets W_{hh}^\top du_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

As mentioned, there are difficulties in training RNNs; however, resorting to second-order methods solves some issues. This thesis includes the implementation of the Hessian-Free, HF, optimization \cite{suts}, which is a second-order method avoiding explicit calculation of the Hessian matrix. Combined with the R-method, the two above algorithms are practically sufficient for calculating the product $Hv$. 

The relation between the Hessian and R-operator $Hv = R(\nabla f(\theta))$ determines the algorithm for calculating the product in terms of the above algorithms, only a forward pass followed by a backward pass. A rigorous treatment can be found in \cite{suts}.

However, as stated in the work \cite{suts}, the approximation called the Gaus-Newton matrix, \(G\), of the Hessian is preferred for stability. There is another reason for using $G$ instead which relates to the "matching" of the cost function with the output function, this makes the $G$ matrix independent of the target $y$ and positive semi definite \cite{Martens2012}. Described in the algorithm below is the product \(Gv\) for some vector $v$, with "matching" cost and output function:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$Ru_t \gets W^v_{hi}i_t + W^v_{hh}h_{t-1} + W_{hh}Rh_{t-1} + b^v_h$}
    \State{$Rh_t \gets a_h'(u_t)Ru_t$}
    \State{$Rz_t \gets W^v_{oh}h_t + W_{oh}Rh_t + b^v_o $}
    \State{$Ro_t \gets a_o'(z_t)Rz_t$}
    \EndFor
    \For{$t$ from $T$ to $1$}
    \State{$Rdz_t \gets a_o'(z_t) R dC(o_t; y_t)/d o_t $}
    \State{$Rdh_t \gets Rdh_t + W_{oh}^\top Rdz_t$}
    \State{$Rdu_t \gets a_h'(u_t)Rdh_t $}
    \State{$Rdb_o \gets Rdb_o + Rdz_t$}
    \State{$Rdb_h \gets Rdb_h + Rdu_t $}
    \State{$RdW_{oh} \gets RdW_{oh} + Rdz_th_t^\top $}
    \State{$RdW_{hh} \gets RdW_{hh} + Rdu_th_{t-1}^\top $}
    \State{$RdW_{hi} \gets RdW_{hi} + Rdu_ti_t^\top$}
    \State{$Rdh_{t-1} \gets W_{hh}^\top Rdu_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

where the "matching" property modifies $a_o'(z_t) = \mathds{1}$ on line $5$ and \[R\frac{dC(o_t; y_t)}{d o_t} = \frac{d^2C(o_t; y_t)}{d^2o_t} Ro_t =  a_o'(z_t)  Ro_t\] as they are formally computed together. While reading the pseudo-code the $R$ notation can be thought of as defining a new symbol and intermediate quantities are pre-computed. The $Gv$ product can be achieved from $Hv$ by simply disregarding derivatives of none differential terms, treating them as "constant" and formally independent of $\theta$. Once more this is further discussed in \cite{suts}.

For second order methods, including the HF approach, other measures need to be incorporated to guarantee stability and convergence. The main constitutent of HF is CG, the conjugate gradient method. For this a sub-objective based on an approximation of the objective $f(\theta)$ is minimized by CG. The iterative minimization, specifically, given a parameter setting $\theta_n$ for a $\theta_{n+1}$ is found by partially optimizing \[q_{\theta_n} (\theta) \equiv M_{\theta_n}(\theta) + \lambda R_{\theta_n}(\theta),\] where \[M_{\theta_n}(\theta) = f(\theta_n) + f'(\theta_n)^\top \delta_n + \frac{\delta_n^\top C_n \delta_n}{2},\] and $C_n$ is an approximation to the curvature of $f$ at $\theta_n$. $\delta_n = \theta - \theta_n$ is the $n^{th}$ step. The $\lambda R_{\theta_n}(\theta)$ is the regularization term where $\lambda$ is a size parameter and $R_{\theta_n}(\theta)$ is a dampening term, penalizing the size of the $\delta_n$ according to Tikhonov-dampening $R_{\theta_n}(\theta) = \norm{\delta}^2/2$. The quadratic function to be minimized is now set-up and the CG algorithm is run accordingly:

\begin{algorithm}
    \caption{CG - Conjugate Gradient, truncated}
    \begin{algorithmic}[1]
        \State{Define quadratic objective $\phi(z) = z^\top Bz/2 + b^\top z$ where $B = C_n + \lambda$ and $b = f'(\theta_n)$, $\nabla \phi(z_i) = B z_i + b$}
        \State{Let $i=0$ and $z_0 = \delta_{n-1}$ and $d_i=d_0=-\nabla \phi (z_0)$}
        \While{truncation criteria is not met}
        \State{Compute step size $\alpha = -\frac{d_i (Bz_i + b)}{d_i^\top B d_i}$}
        \State{Update $z_{i+1} = z_i + \alpha d_i$}
        \State{Update $d_{i+1} = -\nabla \phi(z_{i+1}) + \beta_i d_i$ where $\beta_i = \frac{\nabla \phi(z_{i+1}) B d_i}{d_i^\top B d_i}$}
        \State{$i \gets i+1$}
        \EndWhile
        \State{ \Return{$z_i = \delta_n$}}
    \end{algorithmic}
\end{algorithm}

As can be seen, only the product $Bz$ or $Bd$ is ever needed, thus free of the Hessian. Several different truncation criteria exist, one used in this thesis is a limit in total steps as the accuracy of $M(\delta)$ as a model of $f(\theta_n + \delta)$ will go down even though $f(\theta_n + \delta)$ may be improving still. Another preventive measure implemented is the relative progress which stops CG when \[s_j = \frac{M(z_j) - M(z_{j-k})}{M(z_j) - M(0)} < c.\]  Choosing $c = 0.0001$ and $k= \max (10, j/10)$, recommended in \cite{suts}. Also discussed in \textcite{Martens2012} is the effect of truncation as dampening and possible positive effects on $f$. Utilized in the CG above is the initialization described in \cite{Martens2012} as the previous step $z_0 = \delta_{n-1}$ and not $z_0 = 0$.

A simplified HF algorithm is stated below:

\begin{algorithm}
    \caption{HF - simplified}
    \begin{algorithmic}[1]
        \State{$\delta_0 \gets 0$}
        \State{$ \lambda \gets 50$ from \cite{suts}}
        \State{assign mini-batch for each n, $\{U_n\}$}
        \For{each mini-batch $n = 1,2,...$}
        \State{define $Bv = \frac{1}{\abs{U_n}}\sum_{(x,y)\in U_n} G_f((x,y);\theta_n)v + \lambda v$}
        \State{compute $b = -\frac{1}{\abs{U_n}}\sum_{(x,y)\in U_n} f'((x,y);\theta_n)$}
        \State{find $\delta_{n+1}$ with CG on $z^\top B z/2 + b^\top z $, $z_0 = \delta_{n-1}$}
        \State{adjust $\lambda$ according to Levenburg-Marquardt heuristic}
        \State{adjust $\delta_{n+1}\alpha$, where $\alpha$ is calculated with a back-tracking line search}
        \State{$\theta_{n+1} = \theta_n + \alpha \delta_n$}
        \State{$n \gets n+1$}
        \EndFor
    \end{algorithmic}
\end{algorithm}

The Levenburg-Marquardt, LM, heuristic is simply as follows:

\[ \rho = \frac{f(\theta_n + \delta_n) - f(\theta_n)}{q_{\theta_n} (\delta_n)}\]

and based on $\rho$ adjust $\lambda$ according to

\begin{enumerate}
    \item \textbf{if} $\rho > 3/4 $ \textbf{then} $\lambda\gets 2/3 \lambda$
    \item \textbf{if} $\rho<1/4$ \textbf{then} $\lambda \gets 3/2\lambda$
\end{enumerate}   
    
the size will be adjusted depending on the accuracy of reduction in the approximation $q_{\theta_n}$ in relation to the objective. 

The last line of defense is the standard line searching. Provided that $\delta_n$ is a descent direction $(\delta_n^\top \nabla f(\theta_n) < 0)$ follows that for a small, non-zero value $\alpha$ we will have \[f(\theta_n + \alpha \delta_n) = f(\theta_{n+1}) < f(\theta_n),\] $\delta_n$ will be a descent direction as long as $B$ is positive definite, $\nabla f(\theta_n)$ is computed on the mini-batch and $M$ is optimized partially with CG\cite{Martens2012}. This guarantees that there will be a decrease of $f$ on the mini-batch. The back-tracking can simply be implemented with the below condition, starting with $\alpha = 1$ and repeatedly multiply with $\beta = 1/2$ until:

\[f(\theta_n + \alpha \delta_n) < f(\theta_n) + c \alpha \nabla f(\theta_n)^\top \delta_n\]

where $c$ is a small constant $10^{-2}$.

\section{Monte Carlo Regularization}

As mentioned above, the algorithm itself gathers from the grand canonical ensemble, GCE, simulation. The operators for a node $i$ corresponding to the allowed moves, $D_i$ deletes node $i$, $C_i$ copies node $i$ and $D_{ij}$ deletes the weight between node $i,j$ in the NN. These operators induce energy differences $E_D$, $E_C$ and $E_d$ respectively. 

\begin{algorithm}[h]
    \caption{Monte Carlo regularization}
    \begin{algorithmic}[1]
        \State{Randomly choose to perturb the NN}
        \If{perturb}
        \State{Calculate $E$ on the selection set, $E = \frac{1}{N_S}\sum_n C(f(i^n, \theta);y^n)$ where $N_S$ is the size}
        \State{Operate with each move on the graph, generate all corresponding graphs $i$ and $ij$ for each $C_i$, $D_i$, $D_{ij}$ where $i$ is a node in the hidden layer and $j$ is any node}
        \State{Calculate $\Delta E$ for each allowed move, $E_D$, $E_C$ and $E_d$ on the selection set for each graph $\Delta E_{D_i}$, $\Delta E_{C_i}$, $\Delta E_{d_{ij}}$  }
        \State{For each move, choose the best graph for which corresponding $\Delta E$ is maximised}
        \State{Reject any $\Delta E < 0$}
        \State{Calculate softmax of the resulting $\Delta E$, ergo create probability distribution over these moves}
        \State{Randomly choose one move}
        \State{\Return New NN}
        \EndIf
    \State \Return{Old network}
    \end{algorithmic}
\end{algorithm}

where $f(i^n, \theta)= o^n$ and the superscript $n$ is the sample.

The decision to perturb the network is based on a relaxation of the number of epochs, the later the epoch, the lesser the probability. The relaxation is to avoid a last-minute perturbation. The purpose of computing the difference energy solely on the selection set was, in part, due to the size of the training set being too large and improving generalization; thus, an external set was used. Furthermore, a subset of the selection set was used to evaluate weight deletion as they number many more than the nodes. The rejection can be softened to allow small decreases in performance; however, there is no a priori motivation. The distribution used was softmax; however, a preference for some moves can be established with a more askew distribution, and better reflect the relative $\Delta E$ differences. Finally, randomly choosing one move was to survey the effects of the different moves better.  

The relation to the GCE simulation is the insertion and deletion of nodes. Disregarding the size or the number of nodes of the network, as no favor in size was selected. A possibility is the preference of small networks; however, accurately balancing this in terms of probabilities is for another thesis. Similarly, as with configurational bias, a randomly generated node seems unlikely to produce an improvement, and therefore it is sampled from an existing node. The parameters related to the new node is then slightly, randomly, changed. 