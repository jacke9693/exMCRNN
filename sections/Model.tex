\chapter{Model Description}

This section describes firstly the underlying theory of computational graphs, secondly the RNN structure and lastly the regularization.

\section{Differentiable Computational Graph}

Before introducing artificial neural networks as such, the mathematical foundation needs to be formalized as the regularization, to be well-defined, couples to the underlying theory of graphs. Firstly is the efficient computation of derivatives of any function $F(\theta)$ that can be evaluated with such a graph. 

Considering a graph over $N$ nodes, $1, ..., N$ where $I$ is the subset consisting of the input nodes and $N$ is the output node. Each node $i$ has a set of ancestors $A_i$, determining its input, and a differentiable function $f_i$. For the above an algorithm for evaluating $F(\theta)$ on a computational graph:

\begin{algorithmic}[1]
    \State{Let $\theta$ be distributed between the input nodes $z_i, i \in I$}
    \For{$i=1$ to $N$ if $i\notin I$}
    \State{$x_i \gets \text{concat}_{j\in A_i}z_j$}
    \State{$z_i \gets f_i(x_i)$}
    \EndFor
    \State{\Return{$F(\theta) = z_N$}}
\end{algorithmic}

where every node $z_i$ can be vector valued.

Introducing a training error of the form $C(F(\theta)) = C(z_N)$, $C$ is the cost function and $F(\theta)$ is the models prediction on all samples. The derivative of $C(z_N)$ w.r.t. $\theta$ is given by $F'(\theta)^\top C'(z_N)$. The backpropagation algorithm calculates the product $F'(\theta)^\top v$ for arbitrary vector $v$ with according dimensionality of $z_N$:

\begin{algorithmic}[1]
    \State{$dz_N \gets v$}
    \State{$dz_i \gets 0$ for $i<N$}
    \For{$i = N$ to $1$ if $i \notin I$}
    \State{$dx_i \gets f_i'(x_i)^\top dz_i$}
    \For{$j \in A_i $}
    \State{$dz_j \gets dz_j + \text{unconcat}^jdx_i$}
    \EndFor
    \EndFor
    \State{concatenate $dz_i$ for $i \in I$ onto $d\theta$}
    \State{\Return{$d\theta = F'(\theta)^\top v$ }}
\end{algorithmic}

$\text{unconcat}$ being the inverse of $\text{concat}$ such that: if $x_i = \text{concat}_{j\in A_i} z_j$ then $\text{unconcat}^j x_i = z_j$. 

This formalism allows for easy and automated application of the R-operator. The $R_v$ operator is defined as follows for $R_vx $ denoting the directional derivative of a $\theta$-dependent variable $x$ in the direction $v$:

\[R_vx = \lim_{\epsilon\to 0} \frac{x(\theta + \epsilon v) - x(\theta)}{\epsilon} = \frac{\partial x}{\partial\theta}v\]

As a derivative, the $R_v$ operator obeys the following rules of differentiation:

\begin{enumerate}
    \item $R_v(x+y) = R_vx + R_vy$
    \item $ R_v(xy) = (R_vx)y + (R_vy)x $
    \item $R_v(\gamma(x)) = (R_vx) J_{\gamma}(x)$
\end{enumerate}

where $J_\gamma(x)$ denotes the Jacobian of $\gamma$. As can be recognized these are linearity, the product rule and chain rule respectively. The $v$ will be henceforth suppressed and implied for compact notation, additionally the relation $Rb = b^v$ establishes the parameter $b$ with respect to $v$.

Given the above, computation of $R(F(\theta))$ is described in the following algorithm:

\begin{algorithmic}[1]
    \State{distribute input $v$ across input nodes $Rz_i$ for $i \in I$}
    \For{$i = 1$ to $N$ if $i \notin I$ do}
    \State{$Rx_i \gets \text{concat} Rz_j$}
    \State{$Rz_i \gets f_i'(x_i) R x_i$}
    \EndFor
    \State{\Return{$Rz_N = F'(\theta) v $}}
\end{algorithmic}

The algorithms for differentiation can be proven with structural induction over the graphs and is omitted. The needs for this in the Hessian-Free approach becomes clear with the relation $Hv = R(\nabla f(\theta))$, where $H$ is the Hessian of $f$. 

From two graphs $G_1 = (V_1,E_1)$ and $G_2 = (V_2, E_2)$ a new graph can be generated. The operators, deletion of either node $i$ or weight $i,j$ and the copying of node $i$, required for the moves or changes to the graph, relate to the binary operators in the following way:

\begin{enumerate}
    \item Copying $C_i$: $G_1 \cup G_2 = (V_1 \cup V_2, E_1 \cup V_2)$ where $V_1 = V_2 / V_i$ 
    \item Deletion $D_i $: $G_1 \cap G_2 = (V_1 \cap V_2, E_1 \cap V_2)$ where $V_2 = V_1 / V_i$ or $E_2 = E_1 / E_i$
\end{enumerate}

\section{Recurrent Neural Network}

The recurrent neural network, RNN, is a nonlinear dynamical system mapping sequences to sequences. RNNs are typically structured with one input $i$, one hidden $h$ and one output layer $o$. Being parametrized by three weight matrices and two bias vectors \([W_{hi}, W_{hh}, W_{oh}, b_h, b_o]\) the state concatenation denoted \(\theta\) completely describes the RNN. Additionally an initial hidden state $h_0$ can be included but $h_0$ the initial hidden state is set to the zero vector in this thesis.

The forward propagation through the network of an input sequence \((i_1, i_2, ... , i_T)\) to an output sequence \((o_1, o_2, ..., o_T) \) is performed in the following algorithm:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$u_t \gets W_{hi}i_t + W_{hh}h_{t-1} + b_h$}
    \State{$h_t \gets a_h(u_t)$}
    \State{$z_t \gets W_{oh}h_t + b_o $}
    \State{$o_t \gets a_o(z_t)$}
    \EndFor
\end{algorithmic}

where $h_0 = 0$. $a_h(*)$ and $a_o(*)$ are nonlinear activation functions for the hidden and output layer respectively. The cost function is usually defined as a sum of per-timestep cost:

\[C(o, y) = \sum_{t=1}^{T}C(o_t; y_t)\]

for some target sequence $y_t$. With the cost function a derivative for the RNNs can be computed by the backpropagation through time algorithm, BPTT \cite{RNN1}:

\begin{algorithmic}[1]
    \For{$t$ from $T$ to $1$}
    \State{$do_t \gets a_o'(o_t) \partial C(o_t; y_t)/\partial o_t $}
    \State{$db_o \gets db_o + do_t$}
    \State{$dW_{oh} \gets dW_{oh} + db_oh_t^\top $}
    \State{$dh_t \gets dh_t + W_{oh}^\top do_t$}
    \State{$du_t \gets a_h'(u_t)dh_t $}
    \State{$dW_{hi} \gets dW_{hi} + du_ti_t$}
    \State{$db_h \gets db_h + du_t $}
    \State{$dW_{hh} \gets dW_{hh} + du_th_{t-1}^\top $}
    \State{$ dh_{t-1} \gets W_{hh}^\top du_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

As mentioned there are difficulties in training RNNs, resorting to second order methods, however, solves some issues. Included in this thesis is the Hessian-Free, HF, optimization, which is a second order method avoiding explicit calculation of the Hessian matrix. Combined with the R-method the two above algorithms are essentially sufficient for calculating the product $Hv$. 

The relation between the Hessian and R-operator $Hv = R(\nabla f(\theta))$ determines the algorithm for calculating the product in terms of the above algorithms, simply a forward pass followed by a backward pass. A rigorous treatment can be found in \cite{suts}.

However, as stated in the work \cite{suts}, the approximation called the Gaus-Newton matrix, \(G\), of the Hessian is preferred for stability. There is another reason for using $G$ instead which relates to the "matching" of the cost function with the output function, this makes the $G$ matrix independent of the target $y$ and positive semi definite \cite{Martens2012}. Described in the algorithm below is the product \(Gv\) for some vector $v$, with "matching" cost and output function:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$Ru_t \gets R{W_{hi}}i_t + RW_{hh}h_{t-1} + W_{hh}Rh_{t-1} + Rb_h$}
    \State{$Rh_t \gets a_h'(u_t)Ru_t$}
    \State{$Rz_t \gets RW_{oh}h_t + W_{oh}Rh_t + Rb_o $}
    \State{$Ro_t \gets a_o'(o_t)Rz_t$}
    \EndFor
    \For{$t$ from $T$ to $1$}
    \State{$Rdo_t \gets a_o'(o_t) RdC(o_t; y_t)/do_t $}
    \State{$db_o \gets db_o + do_t$}
    \State{$dW_{oh} \gets dW_{oh} + db_oh_t^\top $}
    \State{$dh_t \gets dh_t + W_{oh}^\top do_t$}
    \State{$du_t \gets a_h'(u_t)dh_t $}
    \State{$dW_{hi} \gets dW_{hi} + du_ti_t$}
    \State{$db_h \gets db_h + du_t $}
    \State{$dW_{hh} \gets dW_{hh} + du_th_{t-1}^\top $}
    \State{$ dh_{t-1} \gets W_{hh}^\top du_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

The $Gv$ product can be achieved from $Hv$ by simply disregarding derivatives of none differential terms, treating them as constant. Once more this is further discussed in \cite{suts}.

For second order methods, including the HF approach, other measures need to be incorporated to guarantee stability and convergence. The main constitutent of HF is CG, the conjugate gradient method. For this a sub-objective based on an approximation of the objective $f(\theta)$ is minimized by CG. The iterative minimization, specifically, given a parameter setting $\theta_n$ for a $\theta_{n+1}$ is found by partially optimizing \[q_{\theta_n} (\theta) \equiv M_{\theta_n}(\theta) + \lambda R_{\theta_n}(\theta),\] where \[M_{\theta_n}(\theta) = f(\theta_n) + f'(\theta_n)^\top \delta_n + \frac{\delta_n^\top C_n \delta_n}{2},\] and $C_n$ is an approximation to the curvature of $f$ at $\theta_n$. $\delta_n = \theta - \theta_n$ is the $n^{th}$ step. The $\lambda R_{\theta_n}(\theta)$ is the regularization term where $\lambda$ is a size parameter and $R_{\theta_n}(\theta)$ is a dampening term, penalizing the size of the $\delta_n$ according to Tikhonov-dampening $R_{\theta_n}(\theta) = \norm{\delta}^2/2$. The quadratic function to be minimized is now set-up and the CG algorithm is run accordingly:

\begin{algorithm}
    \caption{CG - Conjugate Gradient, truncated}
    \begin{algorithmic}[1]
        \State{Define quadratic objective $\phi(z) = z^\top Bz/2 + b^\top z$ where $B = C_n + \lambda$ and $b = f'(\theta_n)$, $\nabla \phi(z_i) = B z_i + b$}
        \State{Let $i=0$ and $z_0 = \delta_{n-1}$ and $d_i=d_0=-\nabla \phi (z_0)$}
        \While{truncation criteria is not met}
        \State{Compute step size $\alpha = -\frac{d_i (Bz_i + b)}{d_i^\top B d_i}$}
        \State{Update $z_{i+1} = z_i + \alpha d_i$}
        \State{Update $d_{i+1} = -\nabla \phi(z_{i+1}) + \beta_i d_i$ where $\beta_i = \frac{\nabla \phi(z_{i+1}) B d_i}{d_i^\top B d_i}$}
        \EndWhile
    \end{algorithmic}
\end{algorithm}

As can be seen, only the product $Bz$ or $Bd$ is ever needed, thus free of the Hessian. Several different truncation criteria exist, one used in this thesis is a limit in total steps as the accuracy of $M(\delta)$ as a model of $f(\theta_n + \delta)$ will go down even though $f(\theta_n + \delta)$ may be improving still. Another preventive measure implemented is the relative progress which stops CG when \[s_j = \frac{M(z_j) - M(z_{j-k})}{M(z_j) - M(0)} < 0.0001.\]  Choosing $k= \max (10, j/10)$, recommended in \cite{suts}. Also discussed in \textcite{Martens2012} is the effect of truncation as dampening and possible positive effects on $f$. Utilized in the CG above is the initialization described in \cite{Martens2012} as the previous step $z_0 = \delta_{n-1}$ and not $z_0 = 0$.

A simplified HF algorithm is stated below:

\begin{algorithm}
    \caption{HF - simplified}
    \begin{algorithmic}[1]
        \State{$\delta_0 \gets 0$}
        \State{$ \lambda \gets 50$ from \cite{suts}}
        \State{assign mini-batch for each n, $\{U_n\}$}
        \For{each mini-batch $n = 1,2,...$}
        \State{define $Bv = \frac{1}{\abs{U_n}}\sum_{(x,y)\in U_n} G_f((x,y);\theta_n)v + \lambda v$}
        \State{compute $b = -\frac{1}{\abs{U_n}}\sum_{(x,y)\in U_n} f'((x,y);\theta_n)$}
        \State{find $\delta_{n+1}$ with CG on $z^\top B z/2 + b^\top z $, $z_0 = \delta_{n-1}$}
        \State{adjust $\lambda$ according to Levenburg-Marquardt heuristic}
        \State{adjust $\delta_{n+1}\alpha$, where $\alpha$ is calculated with a back-tracking line search}
        \EndFor
    \end{algorithmic}
\end{algorithm}

The Levenburg-Marquardt, LM, heuristic is simply as follows:

\[ \rho = \frac{f(\theta_n + \delta_n) - f(\theta_n)}{q_{\theta_n} (\delta_n)}\]

and based on $\rho$ adjust $\lambda$ according to

\begin{algorithmic}[1]
    \If{$\rho > 3/4 $}
    \State{$\lambda\gets 2/3 \lambda$}
    \EndIf
    \If{$\rho<1/4$}
    \State{$\lambda \gets 3/2\lambda$}
    \EndIf
\end{algorithmic}

the size will be adjusted depending on the accuracy of reduction in the approximation $q_{\theta_n}$ in relation to the objective. 

The last line of defense is the standard line searching. Provided that $\delta_n$ is a descent direction $(\delta_n^\top \nabla f(\theta_n) < 0)$ follows that for a small, non-zero value $\alpha$ we will have \[f(\theta_n + \alpha \delta_n) = f(\theta_{n+1}) < f(\theta_n),\] $\delta_n$ will be a descent direction as long as $B$ is positive definite, $\nabla f(\theta_n)$ is computed on the mini-batch and $M$ is optimized partially with CG\cite{Martens2012}. This guarantees that there will be a decrease of $f$ on the mini-batch. The back-tracking can simply be implemented with the below condition, starting with $\alpha = 1$ and repeatedly multiply with $\beta = 1/2$ until:

\[f(\theta_n + \alpha \delta_n) < f(\theta_n) + c \alpha \nabla f(\theta_n)^\top \delta_n\]

where $c$ is a small constant $10^{-2}$.

\section{Monte Carlo Regularization}

As mentioned above, the algorithm itself gathers from the grand canonical ensemble, GCE, simulation. The operators for a node $i$ corresponding to the allowed moves, $D_i$ deletes node $i$, $C_i$ copies node $i$ and $D_{ij}$ deletes the weight between node $i,j$ in the NN. These operators induce energy differences $E_D$, $E_C$ and $E_d$ respectively. 

\begin{algorithm}
    \caption{Monte Carlo regularization}
    \begin{algorithmic}[1]
        \State{Randomly choose to perturb the NN}
        \If{perturb}
        \State{Calculate $E$ on the selection set}
        \State{Calculate $\Delta E$ for each allowed move, $E_D$, $E_C$ and $E_d$ on the selection set}
        \State{For each move, choose the best for which $\Delta E$ is maximised}
        \State{Reject any $\Delta E < 0$}
        \State{Calculate softmax of the resulting $\Delta E$, ergo create probability distribution over these moves}
        \State{Randomly choose one move}
        \State{\Return New NN}
        \EndIf
    \State \Return{Old network}
    \end{algorithmic}
\end{algorithm}

The decision to perturb the network is based on a relaxation of the number of epochs, the later the epoch the lesser the probability. This is to avoid a last minute perturbation. The purpose of computing the difference energy solely on the selection set is due to the size of the training set being to large and to improve generalization, thus an external set was used. The rejection can be softened to allow small decreases in performance, however, a priori there is no motivation for that. The distribution used was softmax, however, a preference for some moves can be established with a more askew distribution, also to better reflect the relative $\Delta E$ differences. Finally, randomly choosing one move was to better survey the effects of the different moves. 

The relation to the GCE simulation is the insertion and deletion of nodes. Disregarding the size or the amount of nodes of the network, as no favor in size was selected. A possibility is the preference of small networks, however, accurately balancing this in terms of probabilities is for another thesis. Similarly as with configurational bias, a randomly generated node seems unlikely to produce an improvement and therefore it is sampled from an existing node. The parameters related to the new node is then slightly, randomly, changed. 