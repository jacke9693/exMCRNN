\chapter{Model Description}

This section firstly describes the underlying theory of computational graphs, secondly the RNN structure, and lastly, the regularization. A lot of this section is based on the work of \textcite{suts}, \textcite{Martens2012} and for computational graphs \textcite{NoceWrig06}.

\section{Differentiable Computational Graph}

Before introducing the theory of artificial RNNs, the mathematical foundation for automatic differentiation needs to be formalized. Firstly is the efficient computation of derivatives of any function $F = F(\theta)$ that can be evaluated with a computational graph, where $\theta$ are the parameters rather than an input pattern. The introduction in the form of computational graphs for these algorithms shows the inductive and algorithmic calculation and evaluation of the derivatives for NNs.

Considering a graph over $N$ nodes, $\{1, ..., N\} = \mathcal{N}$ where $I \subset \mathcal{N}$ is the subset consisting of the input nodes and $O \subset \mathcal{N}$ is the subset of output nodes. Each node $i$ has a set of parents $P_i$, determining its input, and a differentiable function $f_i$. For the above an algorithm for evaluating $F(\theta)$ on a computational graph \cite{suts}:

\begin{algorithmic}[1]
    \State{Let $\theta$ be distributed between the input nodes $z_i, i \in I$}
    \For{$i=1$ to $N$ \textbf{if} $i\notin I$}
    \State{$x_i \gets \text{concat}_{j\in P_i}z_j$}
    \State{$z_i \gets f_i(x_i)$}
    \EndFor
    \State{\Return{$F(\theta) = z_i \mid i \in O $}}
\end{algorithmic}

\noindent
where every node $z_i$ can be vector-valued and $\text{concat}$ is simply the concatenation over the parent set. The formalism relates straightforwardly to the forward propagation of NNs and captures nearly all models in machine learning.

Introducing a training error of the form $C(F(\theta)) = C(z_N)$, $C$ is the cost function and $F(\theta)$ is the model's prediction on all samples. The derivative of $C(z_N)$ w.r.t. $\theta$ is given by $F'(\theta)^\intercal C'(z_N)$. The backpropagation algorithm calculates the product $F'(\theta)^\intercal v$ for arbitrary vector $v$ with according dimensionality of $z_N$ \cite{suts}:

\begin{algorithmic}[1]
    \State{$dz_{i_{i \in O}}\gets v$}
    \State{$dz_i \gets 0$ for $i \notin O$}
    \For{$i = N$ \textbf{to} $1$ \textbf{if} $i \notin I$}
    \State{$dx_i \gets f_i'(x_i)^\intercal dz_i$}
    \For{$j \in P_i $}
    \State{$dz_j \gets dz_j + \text{unconcat}^jdx_i$}
    \EndFor
    \EndFor
    \State{concatenate $dz_i$ for $i \in I$ onto $d\theta$}
    \State{\Return{$d\theta = F'(\theta)^\intercal v$ }}
\end{algorithmic}

\noindent
$\text{unconcat}$ being the inverse of $\text{concat}$ such that: if $x_i = \text{concat}_{j\in P_i} z_j$ then $\text{unconcat}^j x_i = z_j$. If $v = C'(z_N)$, the algorithm shows the accumulation of error, at each node, which propagates backwards. The two algorithms combined with a training error allows for application in the form of a neural network. 

This formalism allows for the easy and automated application of the R-operator \cite{pepe}. The $R_v$ operator is defined as follows for $R_vx $ denoting the directional derivative of a $\theta$-dependent variable $x$ in the direction $v$:

\[R_vx = \lim_{\epsilon\to 0} \frac{x(\theta + \epsilon v) - x(\theta)}{\epsilon} = \frac{\partial x}{\partial\theta}v\]

\noindent
As a derivative, the $R_v$ operator obeys the following rules of differentiation:

\begin{enumerate}
    \item $R_v(x+y) = R_vx + R_vy$
    \item $ R_v(xy) = (R_vx)y + (R_vy)x $
    \item $R_v(\gamma(x)) = (R_vx) J_{\gamma}(x)$
\end{enumerate}

\noindent
where $J_\gamma(x)$ denotes the Jacobian of $\gamma$. As can be recognized, these are linearity, the product rule, and chain rule, respectively. The $v$ will be henceforth suppressed and implied for compact notation; additionally, the relation $Rb = b^v$ establishes the parameter $b$ with respect to $v$.

Given the above, computation of $R(F(\theta)) = F'(\theta)^\intercal v$, constituting a forward mode differentiation, is described in the following algorithm \cite{suts}:

\begin{algorithmic}[1]
    \State{distribute input $v$ across input nodes $Rz_i$ for $i \in I$}
    \For{$i = 1$ \textbf{to} $N$ \textbf{if} $i \notin I$}
    \State{$Rx_i \gets \text{concat}_{j\in P_i} Rz_j$}
    \State{$Rz_i \gets f_i'(x_i) R x_i$}
    \EndFor
    \State{\Return{$Rz_N = F'(\theta)^\intercal v $}}
\end{algorithmic}

\noindent
analogously calculating the derivative as in backpropagation. The algorithms for differentiation, backward and forwards, can be proven with structural induction over the graphs and are omitted. 

The needs for this in the Hessian-free approach becomes clear with the relation $Hv = R(\nabla f(\theta))$, as \[H(\theta)v = \lim_{\epsilon\to 0} \frac{\nabla h(\theta + \epsilon v) - \nabla h(\theta)}{\epsilon} \] where $H$ is the Hessian of $f$. The relation for a single input $x$ and target $y$, using an semi-definite approximation explained in \cite{Martens2012}:

\[H(\theta) \approx J^\intercal H_C J \]

\noindent
where $J$ denotes the Jacobian of $f$ w.r.t. $\theta$ and $H_C$ is the cost function $C(y, x)$ Hessian, w.r.t. $y$ evaluated at $y=f(x,\theta)$. Wherein multiplying with a vector $v$ can then be calculated by forward pass of the R-method for $Jv$, multiplication of $H_C(Jv)$ and a backward pass for $J^\intercal(H_CJv)$.

\section{Structural Graph Operators}

The update of the graph structure can be properly formalized. From two graphs $G_1 = (V_1,E_1)$, where $V_1$ are a set of vertices and $E_1$ a set of edges, and $G_2 = (V_2, E_2)$ a new graph can be generated. The operators, $D_i$ deletion of either node $v_i \in V_1$ or $D_{ij}$ deletion of weight $e_{ij} \in E_1$ and the copying $C_i$ of node $v_i \in V_1$, required for the updates of the graph, relate to the binary operators of graphs in the following way:

\begin{enumerate}
    \item Copying $C_i$, if $v_i \in V_1$: $G_1 \cup G_2 / G_1 \cap G_2$ where $V_2 = \{v'_i\} \cup V_1$ and $E_2 = \{e'_{ij}, e'_{ji}\}$ $\forall j$ $\mid v_j \in V_1$
    \item Deletion $D_i $, if $v_i \in V_1$: $G_1 \cap G_2 = (V_1 \cap V_2, E_1 \cap E_2)$ where $V_2 = V_1 / v_i$ and $E_2 = E_1 / \{e_{ij}, e_{ji}\} $ $ \forall j \mid v_j \in V_1 $
    \item Deletion $D_{ij}$, if $e_{ij} \in E_1$: $G_1 \cap G_2 = (V_1 \cap V_2, E_1 \cap E_2)$ where $V_1 = V_2$ and $E_2 = E_1 / e_{ij}$
\end{enumerate}

\noindent
The $'$ is defined to be a slight perturbation of the associated values in the computational graph such that $v'_i \notin V_1 \cap V_2$. And the union is defined to be $G_1 \cup G_2 = (V_1 \cup V_2, E_1 \cup E_2)$.

\section{Recurrent Neural Network}

The RNN is a nonlinear dynamical system mapping sequences to sequences. RNNs are typically structured with one input $i$, one hidden $h$, and one output layer $o$. Being parametrized by three weight matrices and two bias vectors \([W_{hi}, W_{hh}, W_{oh}, b_h, b_o]\) the state concatenation denoted \(\theta\) completely describes the RNN. Additionally, an initial hidden state $h_0$ can be included, but $h_0$ the initial hidden state is set to the zero vector in this thesis.

The forward propagation through the network of an input sequence \((i_1, i_2, ... , i_T)\) to an output sequence \((o_1, o_2, ..., o_T) \) is performed in the following algorithm \cite{suts}:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$u_t \gets W_{hi}i_t + W_{hh}h_{t-1} + b_h$}
    \State{$h_t \gets a_h(u_t)$}
    \State{$z_t \gets W_{oh}h_t + b_o $}
    \State{$o_t \gets a_o(z_t)$}
    \EndFor
\end{algorithmic}

\noindent
where $h_0 = 0$. $a_h(*)$ and $a_o(*)$ are nonlinear activation functions for the hidden and output layer respectively. The input is distributed over the input nodes, and the information propagates forward through the RNN, collecting at the output nodes creating an output sequence. The relation to the graph formalism and evaluation of a function $F(\theta)$ is clear. The cost function is usually defined as a sum of per-timestep cost:

\[C(o, y) = \sum_{t=1}^{T}C(o_t; y_t)\]

\noindent
for some target sequence $y_t$. With the cost function a derivative for the RNNs can be computed by the BPTT \cite{RNN1}, and format from \cite{suts}:

\begin{algorithmic}[1]
    \For{$t$ from $T$ to $1$}
    \State{$dz_t \gets a_o'(z_t) d C(o_t; y_t)/d o_t $}
    \State{$dh_t \gets dh_t + W_{oh}^\intercal dz_t$}
    \State{$du_t \gets a_h'(u_t)dh_t $}
    \State{$db_o \gets db_o + dz_t$}
    \State{$db_h \gets db_h + du_t $}
    \State{$dW_{oh} \gets dW_{oh} + dz_th_t^\intercal $}
    \State{$dW_{hh} \gets dW_{hh} + du_th_{t-1}^\intercal $}
    \State{$dW_{hi} \gets dW_{hi} + du_ti_t^\intercal$}
    \State{$ dh_{t-1} \gets W_{hh}^\intercal du_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

\noindent
the name relates to the rewinding of the forward propagation. With the pre-evaluated forward propagation related quantities, the error is pushed back through the NN, and the derivative is calculated. As can be seen, backpropagation through time starts from $t=T$ down to one.

As mentioned, there are difficulties in training RNNs; however, resorting to second-order methods solves some issues. This thesis includes the implementation of the HF optimization \cite{suts} in algorithm \ref{HF}, which is a second-order method avoiding explicit calculation of the Hessian matrix. Combined with the R-method, the two above algorithms are practically sufficient for calculating the product $Hv$. 

The relation between the Hessian and R-operator $Hv = R(\nabla f(\theta))$ determines the algorithm for calculating the product in terms of the above algorithms, only a forward pass followed by a backward pass. A rigorous treatment can be found in \cite{suts}.

However, as stated in the work \cite{suts}, the approximation called the Gaus-Newton matrix, \(G\), of the Hessian is preferred for stability. There is another reason for using $G$ instead which relates to the "matching" of the cost function with the output function, this makes the $G$ matrix independent of the target $y$ and positive semi-definite \cite{Martens2012}. Described in the algorithm below is the product \(Gv \approx R(\nabla f(\theta))\) for some vector $v$, with "matching" cost and output function:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$Ru_t \gets W^v_{hi}i_t + W^v_{hh}h_{t-1} + W_{hh}Rh_{t-1} + b^v_h$}
    \State{$Rh_t \gets a_h'(u_t)Ru_t$}
    \State{$Rz_t \gets W^v_{oh}h_t + W_{oh}Rh_t + b^v_o $}
    \State{$Ro_t \gets a_o'(z_t)Rz_t$}
    \EndFor
    \For{$t$ from $T$ to $1$}
    \State{$Rdz_t \gets a_o'(z_t) R dC(o_t; y_t)/d o_t $}
    \State{$Rdh_t \gets Rdh_t + W_{oh}^\intercal Rdz_t$}
    \State{$Rdu_t \gets a_h'(u_t)Rdh_t $}
    \State{$Rdb_o \gets Rdb_o + Rdz_t$}
    \State{$Rdb_h \gets Rdb_h + Rdu_t $}
    \State{$RdW_{oh} \gets RdW_{oh} + Rdz_th_t^\intercal $}
    \State{$RdW_{hh} \gets RdW_{hh} + Rdu_th_{t-1}^\intercal $}
    \State{$RdW_{hi} \gets RdW_{hi} + Rdu_ti_t^\intercal$}
    \State{$Rdh_{t-1} \gets W_{hh}^\intercal Rdu_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

\noindent
where the "matching" property modifies $a_o'(z_t) = \mathds{1}$ on line $5$ and \[R\frac{dC(o_t; y_t)}{d o_t} = \frac{d^2C(o_t; y_t)}{d^2o_t} Ro_t =  a_o'(z_t)  Ro_t\] as they are formally computed together. While reading the pseudo-code the $R$ notation can be thought of as defining a new symbol and intermediate quantities are pre-computed. As mentioned when introducing the R-method, the matrix product $Gv = J^\intercal H_C J v$ can be calculated with first the forward pass $Jv = R(f(x, \theta))$, in the first \textbf{for} loop in the above code. Then multiplication with $H_C$ is simple due to the "matching" property. Lastly the backpropagation is used to calculate multiplication of $J^\intercal$, as shown in the latter \textbf{for} loop in the above code. The $Gv$ product can be achieved from $Hv$ by simply disregarding derivatives of none differential terms, treating them as "constant" and formally independent of $\theta$. On a closer look, the forward and backpropagation can be seen. Once more this is further discussed in \cite{suts}.

For second order methods, including the HF approach in algorithm \ref{HF}, other measures need to be incorporated to guarantee stability and convergence. The main constitutent of HF is the conjugate gradient method (CG), algorithm \ref{CG}. For this a sub-objective based on an approximation of the objective $f(\theta)$ is minimized by CG. The iterative minimization, specifically, given a parameter setting $\theta_n$ for an updated $\theta_{n+1}$ is found by partially optimizing \[q_{\theta_n} (\theta) \equiv M_{\theta_n}(\theta) + \lambda R_{\theta_n}(\theta),\] where \[M_{\theta_n}(\theta) = f(\theta_n) + f'(\theta_n)^\intercal \delta_n + \frac{\delta_n^\intercal C_n \delta_n}{2},\] and $C_n$ is an approximation to the curvature of $f$ at $\theta_n$. $\delta_n = \theta - \theta_n$ is the $n^{th}$ step. The $\lambda R_{\theta_n}(\theta)$ is the regularization term where $\lambda$ is a size parameter and $R_{\theta_n}(\theta)$ is a dampening term, penalizing the size of the $\delta_n$ according to Tikhonov-dampening $R_{\theta_n}(\theta) = \norm{\delta}^2/2$. The quadratic function to be minimized is now set-up and the CG algorithm is run accordingly:

\begin{algorithm}
    \caption{CG - Conjugate Gradient, truncated}
    \label[alg]{CG}
    \begin{algorithmic}[1]
        \State{Define quadratic objective $\phi(z) = z^\intercal Bz/2 + b^\intercal z$ where $B = C_n + \lambda$ and $b = f'(\theta_n)$, $\nabla \phi(z_i) = B z_i + b$}
        \State{Let $i=0$ and $z_0 = \delta_{n-1}$ and $d_i=d_0=-\nabla \phi (z_0)$}
        \While{truncation criteria is not met}
        \State{Compute step size $\alpha = -\frac{d_i (Bz_i + b)}{d_i^\intercal B d_i}$}
        \State{Update $z_{i+1} = z_i + \alpha d_i$}
        \State{Update $d_{i+1} = -\nabla \phi(z_{i+1}) + \beta_i d_i$ where $\beta_i = \frac{\nabla \phi(z_{i+1}) B d_i}{d_i^\intercal B d_i}$}
        \State{$i \gets i+1$}
        \EndWhile
        \State{ \Return{$z_i = \delta_n$}}
    \end{algorithmic}
\end{algorithm}

As can be seen, only the product $Bz$ or $Bd$ is ever needed, thus free of the Hessian. Several different truncation criteria exist, one used in this thesis is a limit in total steps as the accuracy of $M(\delta)$ as a model of $f(\theta_n + \delta)$ will go down even though $M(\delta)$ may be improving still. Another preventive measure implemented is the relative progress which stops CG when \[s_j = \frac{M(z_j) - M(z_{j-k})}{M(z_j) - M(0)} < c.\]  Choosing $c = 10^{-4}$ and $k= \max (10, j/10)$, recommended in \cite{suts}. Also discussed in \textcite{Martens2012} is the effect of truncation as dampening and possible positive effects on $f$. Utilized in the CG above is the initialization described in \cite{Martens2012} as the previous step $z_0 = \delta_{n-1}$ and not $z_0 = 0$. A simplified HF algorithm is stated below, where heuristic parameters are preset:

\begin{algorithm}
    \caption{HF - simplified}
    \label[alg]{HF}
    \begin{algorithmic}[1]
        \State{$\delta_0 \gets 0$}
        \State{$ \lambda \gets 50$ from \cite{suts}}
        \State{assign mini-batch for each n, $\{U_n\}$}
        \For{each mini-batch $n = 1,2,...$}
        \State{define $Bv = \frac{1}{\abs{U_n}}\sum_{(x,y)\in U_n} G_f((x,y);\theta_n)v + \lambda v$}
        \State{compute $b = -\frac{1}{\abs{U_n}}\sum_{(x,y)\in U_n} f'((x,y);\theta_n)$}
        \State{find $\delta_{n+1}$ with CG on $z^\intercal B z/2 + b^\intercal z $, $z_0 = \delta_{n-1}$}
        \State{adjust $\lambda$ according to Levenburg-Marquardt heuristic}
        \State{adjust $\delta_{n+1}\alpha$, where $\alpha$ is calculated with a back-tracking line search}
        \State{$\theta_{n+1} = \theta_n + \alpha \delta_n$}
        \State{$n \gets n+1$}
        \EndFor
    \end{algorithmic}
\end{algorithm}

The Levenburg-Marquardt (LM) heuristic is simply as follows:

\[ \rho = \frac{f(\theta_n + \delta_n) - f(\theta_n)}{q_{\theta_n} (\delta_n)}\]

\noindent
and based on $\rho$ adjust $\lambda$ according to

\begin{enumerate}
    \item \textbf{if} $\rho > 3/4 $ \textbf{then} $\lambda\gets 2/3 \lambda$
    \item \textbf{if} $\rho<1/4$ \textbf{then} $\lambda \gets 3/2\lambda$
\end{enumerate} 

\noindent
the size will be adjusted depending on the accuracy of reduction in the approximation $q_{\theta_n}$ in relation to the objective. 

The last line of defense is the standard line search. Provided that $\delta_n$ is a descent direction $(\delta_n^\intercal \nabla f(\theta_n) < 0)$ it follows that for a small, non-zero value $\alpha$ we will have \[f(\theta_n + \alpha \delta_n) = f(\theta_{n+1}) < f(\theta_n),\] $\delta_n$ will be a descent direction as long as $B$, the damped GN matrix, is positive definite, $\nabla f(\theta_n)$ is computed on the mini-batch and $M$ is optimized partially with CG\cite{Martens2012}. This guarantees that there will be a decrease of $f$ on the mini-batch. The back-tracking can simply be implemented with the below condition, starting with $\alpha = 1$ and repeatedly multiply with $\beta = 1/2$ until:

\[f(\theta_n + \alpha \delta_n) < f(\theta_n) + c \alpha \nabla f(\theta_n)^\intercal \delta_n\]

where $c = 10^{-2}$ is set.

\section{Monte Carlo Regularization}

The regularization MCR, to be well-defined, couples to the underlying theory of graphs, which will be discussed next. As mentioned above, the MCR in algorithm \ref{MCR} gathers from the grand canonical ensemble (GCE) simulation. The operators for a node $i$ corresponding to the allowed moves, $D_i$ deletes node $i$, $C_i$ copies node $i$ and $D_{ij}$ deletes the weight between node $i,j$ in the NN. These operators induce energy differences $E_D$, $E_C$ and $E_d$ respectively. The reason for disregarding the duplication of weights is their additive property, effectively doubling the strength of the weight. In this section, "energy" refers to the cost.

\begin{algorithm}[h]
    \caption{Monte Carlo regularization}
    \label[alg]{MCR}
    \begin{algorithmic}[1]
        \State{Randomly choose to perturb the NN}
        \If{perturb}
        \State{Calculate $E$ on the selection set, $E = \frac{1}{N_S}\sum_n C(f(i^n, \theta);y^n)$ where $N_S$ is the size}
        \State{Operate with each move on the graph, generate all corresponding graphs $i$ and $ij$ for each $C_i$, $D_i$, $D_{ij}$ where $i$ is a node in the hidden layer and $j$ is any node}
        \State{Calculate $\Delta E$ for each allowed move, $E_D$, $E_C$ and $E_d$ on the selection set for each graph $\Delta E_{D_i}$, $\Delta E_{C_i}$, $\Delta E_{d_{ij}}$  }
        \State{For each move, choose the best graph for which corresponding $\Delta E$ is maximised}
        \State{Reject any $\Delta E < 0$}
        \State{Calculate softmax of the resulting $\Delta E$, ergo create probability distribution over these moves}
        \State{Randomly choose one move}
        \State{\Return New NN}
        \EndIf
    \State \Return{Old network}
    \end{algorithmic}
\end{algorithm}

\noindent
Where in \ref{MCR} $f(i^n, \theta)= o^n$ and the superscript $n$ is the sample.

The decision to perturb the network is based on a relaxation of the number of epochs, the later the epoch, the lesser the probability. The relaxation is to avoid a last-minute perturbation. The purpose of computing the energy difference solely on the selection set was, in part, due to the size of the training set being too large and improving generalization; thus, an external set was used. Furthermore, a subset of the selection set was used to evaluate weight deletion as they number many more than the nodes. The rejection can be softened to allow small decreases in performance; however, there is no a priori motivation. The distribution used was softmax; however, a preference for some moves can be established with a more askew distribution, and better reflect the relative $\Delta E$ differences. Finally, randomly choosing one move was to survey the effects of the different moves better.  

The relation to the GCE simulation is the insertion and deletion of nodes. Similarly, as with configurational bias, a randomly generated node seems unlikely to produce an improvement, and therefore it is sampled from an existing node. The parameters related to the new node is then slightly, randomly, changed. However, in contrast to GCE, no favor in size is selected, and the number of nodes of the network is not implemented to drift.