\chapter{Model Description}

This section describes firstly the RNN structure and secondly the regularization.

\section{Recurrent Neural Network}

The recurrent neural network, RNN, is a nonlinear dynamical system mapping sequences to sequences. RNNs are typically structured with one input $i$, one hidden $h$ and one output layer $o$. Being parametrized by three weight matrices and two bias vectors \([W_{hi}, W_{hh}, W_{oh}, b_h, b_o]\) the state concatenation denoted \(\theta\) completely describes the RNN. Additionally an initial hidden state $h_0$ can be included but $h_0$ the initial hidden state is set to the zero vector in this thesis.

The forward propagation through the network of an input sequence \((i_1, i_2, ... , i_T)\) to an output sequence \((o_1, o_2, ..., o_T) \) is performed in the following algorithm:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$u_t \gets W_{hi}i_t + W_{hh}h_{t-1} + b_h$}
    \State{$h_t \gets a_h(u_t)$}
    \State{$z_t \gets W_{oh}h_t + b_o $}
    \State{$o_t \gets a_o(z_t)$}
    \EndFor
\end{algorithmic}

where $h_0 = 0$. $a_h(*)$ and $a_o(*)$ are nonlinear activation functions for the hidden and output layer respectively. The cost function is usually defined as a sum of per-timestep cost:

\[C(o, y) = \sum_{t=1}^{T}C(o_t; y_t)\]

for some target sequence $y_t$. With the cost function a derivative for the RNNs can be computed by the backpropagation through time algorithm, BPTT \cite{RNN1}:

\begin{algorithmic}[1]
    \For{$t$ from $T$ to $1$}
    \State{$do_t \gets a_o'(o_t) \partial C(o_t; y_t)/\partial o_t $}
    \State{$db_o \gets db_o + do_t$}
    \State{$dW_{oh} \gets dW_{oh} + db_oh_t^\top $}
    \State{$dh_t \gets dh_t + W_{oh}^\top do_t$}
    \State{$du_t \gets a_h'(u_t)dh_t $}
    \State{$dW_{hi} \gets dW_{hi} + du_ti_t$}
    \State{$db_h \gets db_h + du_t $}
    \State{$dW_{hh} \gets dW_{hh} + du_th_{t-1}^\top $}
    \State{$ dh_{t-1} \gets W_{hh}^\top du_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

As mentioned there are difficulties in training RNNs, resorting to second order methods, however, solves some issues. Included in this thesis is the Hessian-Free, HF, optimization, which is a second order method avoiding explicit calculation of the Hessian matrix. Combined with the R-method the two above algorithms are essentially sufficient for calculating the product $Hv$. The $R_v$ operator is defined as follows for $R_vx $ denoting the directional derivative of a $\theta$-dependent variable $x$ in the direction $v$:

\[R_vx = \lim_{\epsilon\to 0} \frac{x(\theta + \epsilon v) - x(\theta)}{\epsilon} = \frac{\partial x}{\partial\theta}v\]

As a derivative, the $R_v$ operator obeys the following rules of differentiation:

\begin{enumerate}
    \item $R_v(x+y) = R_vx + R_vy$
    \item $ R_v(xy) = (R_vx)y + (R_vy)x $
    \item $R_v(\gamma(x)) = (R_vx) J_{\gamma}(x)$
\end{enumerate}

where $J_\gamma(x)$ denotes the Jacobian of $\gamma$. As can be recognized these are linearity, the product rule and chain rule respectively. The $v$ will be henceforth suppressed and implied for compact notation, additionally the relation $R_vb = b^v$ establishes the parameter $b$ with respect to $v$. The relation between the Hessian and R-operator $Hv = R(\nabla(\theta))$ determines the algorithm for calculating the product in terms of the above algorithms, simply a forward pass followed by a backward pass. A rigorous treatment can be found in \cite{suts}.

However, as stated in the work \cite{suts}, the approximation called the Gaus-Newton matrix, \(G\), of the Hessian is preferred for stability. There is another reason for using $G$ instead which relates to the "matching" of the cost function with the output function, this makes the $G$ matrix independent of the target $y$ and positive semi definite \cite{Martens2012}. Described in the algorithm below is the product \(Gv\) for some vector $v$, with "matching" cost and output function:

\begin{algorithmic}[1]
    \For{$t$ from $1$ to $T$}
    \State{$Ru_t \gets R{W_{hi}}i_t + RW_{hh}h_{t-1} + W_{hh}Rh_{t-1} + Rb_h$}
    \State{$Rh_t \gets a_h'(u_t)Ru_t$}
    \State{$Rz_t \gets RW_{oh}h_t + W_{oh}Rh_t + Rb_o $}
    \State{$Ro_t \gets a_o'(o_t)Rz_t$}
    \EndFor
    \For{$t$ from $T$ to $1$}
    \State{$Rdo_t \gets a_o'(o_t) RdC(o_t; y_t)/do_t $}
    \State{$db_o \gets db_o + do_t$}
    \State{$dW_{oh} \gets dW_{oh} + db_oh_t^\top $}
    \State{$dh_t \gets dh_t + W_{oh}^\top do_t$}
    \State{$du_t \gets a_h'(u_t)dh_t $}
    \State{$dW_{hi} \gets dW_{hi} + du_ti_t$}
    \State{$db_h \gets db_h + du_t $}
    \State{$dW_{hh} \gets dW_{hh} + du_th_{t-1}^\top $}
    \State{$ dh_{t-1} \gets W_{hh}^\top du_t$}
    \EndFor
    \State \Return{$d\theta = [dW_{hi}, dW_{hh}, dW_{oh}, db_h, db_o]$}
\end{algorithmic}

The $Gv$ product can be achieved from $Hv$ by simply disregarding derivatives of none differential terms, treating them as constant. Once more this is further discussed in \cite{suts}.

For second order methods, including the HF approach, other measures need to be incorporated to guarantee stability and convergence. The main constitutent of HF is CG, the conjugate gradient method. For this a sub-objective based on an approximation of the objective $f(\theta)$ is minimized by CG. The iterative minimization, specifically, given a parameter setting $\theta_n$ for a $\theta_{n+1}$ is found by partially optimizing \[q_{\theta_n} (\theta) \equiv M_{\theta_n}(\theta) + \lambda R_{\theta_n}(\theta),\] where \[M_{\theta_n}(\theta) = f(\theta_n) + f'(\theta_n)^\top \delta_n + \frac{\delta_n^\top C_n \delta_n}{2},\] and $C_n$ is an approximation to the curvature of $f$ at $\theta_n$. $\delta_n = \theta - \theta_n$ is the $n^{th}$ step. The $\lambda R_{\theta_n}(\theta)$ is the regularization term where $\lambda$ is a size parameter and $R_{\theta_n}(\theta)$ is a dampening term, penalizing the size of the $\delta_n$ according to Tikhonov-dampening $R_{\theta_n}(\theta) = \norm{\delta}^2/2$. The quadratic function to be minimized is now set-up and the CG algorithm is run accordingly:

\begin{algorithm}
    \caption{CG - Conjugate Gradient, truncated}
    \begin{algorithmic}[1]
        \State{Define quadratic objective $\phi(z) = z^\top Bz/2 + b^\top z$ where $B = C_n + \lambda$ and $b = f'(\theta_n)$, $\nabla \phi(z_i) = B z_i + b$}
        \State{Let $i=0$ and $z_0 = \delta_{n-1}$ and $d_i=d_0=-\nabla \phi (z_0)$}
        \While{truncation criteria is not met}
        \State{Compute step size $\alpha = -\frac{d_i (Bz_i + b)}{d_i^\top B d_i}$}
        \State{Update $z_{i+1} = z_i + \alpha d_i$}
        \State{Update $d_{i+1} = -\nabla \phi(z_{i+1}) + \beta_i d_i$ where $\beta_i = \frac{\nabla \phi(z_i) B d_i}{d_i^\top B d_i}$}
        \EndWhile
    \end{algorithmic}
\end{algorithm}

As can be seen, only the product $Bz$ or $Bd$ is ever needed. Several different truncation criteria exist, one used in this thesis is a limit in total steps as the accuracy of $M(\delta)$ as a model of $f(\theta_n + \delta)$ will go down even though $f(\theta_n + \delta)$ may be improving still. Another preventive measure implemented is the relative progress which stops CG when \[s_j = \frac{M(z_j) - M(z_{j-k})}{M(z_j) - M(0)} < 0.0001.\]  Choosing $k= \max (10, j/10)$, recommended in \cite{suts}. Also discussed in \textcite{Martens2012} is the effect of truncation as dampening and possible positive effects on $f$. Utilized in the CG above is the initialization described in \cite{Martens2012} as the previous step $z_0 = \delta_{n-1}$ and not $z_0 = 0$.

A simplified HF algorithm is stated below:

\begin{algorithm}
    \caption{HF - simplified}
    \begin{algorithmic}[1]
        \State{$\delta_0 \gets 0$}
        \State{$ \lambda \gets 50$ from \cite{suts}}
        \State{assign mini-batch for each n, $\{U_n\}$}
        \For{each mini-batch $n = 1,2,...$}
        \State{define $Bv = \frac{1}{\abs{U_n}}\sum_{(x,y)\in U_n} G_f((x,y);\theta_n)v + \lambda v$}
        \State{compute $b = -\frac{1}{\abs{U_n}}\sum_{(x,y)\in U_n} f'((x,y);\theta_n)$}
        \State{find $\delta_{n+1}$ with CG on $z^\top B z/2 + b^\top z $, $z_0 = \delta_{n-1}$}
        \State{adjust $\lambda$ according to Levenburg-Marquardt heuristic}
        \State{adjust $\delta_{n+1}\alpha$, where $\alpha$ is calculated with a back-tracking line search}
        \EndFor
    \end{algorithmic}
\end{algorithm}

The Levenburg-Marquardt, LM, heuristic is simply as follows:

\[ \rho = \frac{f(\theta_n + \delta_n) - f(\theta_n)}{q_{\theta_n} (\delta_n)}\]

and based on $\rho$ adjust $\lambda$ according to

\begin{algorithmic}[1]
    \If{$\rho > 3/4 $}
    \State{$\lambda\gets 2/3 \lambda$}
    \EndIf
    \If{$\rho<1/4$}
    \State{$\lambda \gets 3/2\lambda$}
    \EndIf
\end{algorithmic}

the size will be adjusted depending on the accuracy of reduction in the approximation $q_{\theta_n}$ in relation to the objective. 

The last line of defense is the standard line searching. Provided that $\delta_n$ is a descent direction $(\delta_n^\top \nabla f(\theta_n) < 0)$ follows that for a small, non-zero value $\alpha$ we will have \[f(\theta_n + \alpha \delta_n) = f(\theta_{n+1}) < f(\theta_n),\] $\delta_n$ will be a descent direction as long as $B$ is positive definite, $\nabla f(\theta_n)$ is computed on the mini-batch and $M$ is optimized partially with CG\cite{Martens2012}. This guarantees that there will be a decrease of $f$ on the mini-batch. The back-tracking can simply be implemented with the below condition, starting with $\alpha = 1$ and repeatedly multiply with $\beta = 1/2$ until:

\[f(\theta_n + \alpha \delta_n) < f(\theta_n) + c \alpha \nabla f(\theta_n)^\top \delta_n\]

where $c$ is a small constant $10^{-2}$.

\section{Monte Carlo Regularization}

As mentioned above, the algorithm itself gathers from the grand canonical ensemble simulation. The operators corresponding to the allowed moves, $D_i$ deletes node $i$, $C_i$ copies node $i$ and $D_{ij}$ deletes the weight between node $i,j$ in the NN. These operators induce energy differences $E_D$, $E_C$ and $E_d$ respectively. 

\begin{algorithm}
    \caption{Monte Carlo regularization}
    \begin{algorithmic}[1]
        \State{Randomly choose to perturb the NN}
        \If{perturb}
        \State{Calculate $E$ on the selection set}
        \State{Calculate $\Delta E$ for each allowed move, $E_D$, $E_C$ and $E_d$ on the selection set}
        \State{Reject any $\Delta E < 0$}
        \State{Calculate softmax of the resulting $\Delta E$, ergo create probability distribution over these moves}
        \State{Randomly choose one move}
        \EndIf
    \end{algorithmic}
\end{algorithm}

The decision to perturb the network is based on a relaxation of the number of epochs, the later the epoch the lesser the probability.