\chapter{Discussion}

As shown in the results in figure \ref{acc}, the algorithm performs well for small to medium-sized NN. For the larger NNs, the increased size makes the induced changes relatively smaller; a remedy could be to perform multiple perturbations. For the minimal NNs, the regularization was not so active as all moves or changes were rejected. The inactivity hints that, as expected, the small NNs are slimmed and do not benefit from pruning or growing. 

As can be seen in the average \ref{avg} and the error plot \ref{acc} and table \ref{acc_t}, the regularization seems to smooth the effects of the size of the NN. The $Es$ figure \ref{E} and selection plot \ref{E_sel} are very rich in data and are hard to draw any conclusions from, the generated average \ref{avg} and standard deviation \ref{std} from the $Es$ and $E_{sel}$ provide insight. 

To avoid the variation and increase stability seen in the cross-entropy figure \ref{E}, \ref{E_sel} and \ref{std}, bigger mini-batches could be used, but this relates to the HF approach rather than the regularization. A way to combat the variation more akin to the regularization could be to employ algorithms employed in micro or canonical ensembles, rather than a grand canonical ensemble, further discussed in the conclusions.

The equal treatment of the different moves is not explored, and neither is the algorithm's activity. Simplifying and leaving out the relaxation could benefit these studies. 