\chapter{Discussion}

As shown in the results in Fig. \ref{acc}, the algorithm performs well for small to medium-sized RNN. For the larger NNs, the increased size makes the updates relatively smaller as only one node or edge updates are allowed; a remedy could be to perform multiple perturbations after another. For the minimal initial RNNs, the regularization was not so active as all moves or changes were rejected due to adverse effects on the cross-entropy. The inactivity and rejection hints that, as expected, the small NNs are slimmed and do not benefit from pruning or growing as deemed by the update induced local energy differences. A possibility is a preference in probabilities of small networks; however, accurately balancing this in terms of probabilities is for another thesis. So no regard is taken in MCR to the overall current size of the hidden layer size or the relative size difference in the update.

As shown in the average Fig. \ref{avg} and the error plot Fig. \ref{acc} and Tbl. \ref{acc_t} the regularization seems to smooth the effects of the size of the RNN. The Fig. \ref{E} and selection data set plot Fig. \ref{E_sel} are very rich in data and are hard to draw any conclusions from, the generated average Fig. \ref{avg} and standard deviation Fig. \ref{std} provide insight. The average in Fig. \ref{avg} is indicative of a general performance boost with MCR.

To avoid the variation and increase stability seen in the cross-entropy Fig. \ref{E}, \ref{E_sel} and \ref{std}, bigger mini-batches could be used, but this relates to the HF approach rather than the regularization. Gathering from other sampling algorithms used in physics, in other ways to combat the variation could be to employ algorithms utilized in micro or canonical ensembles, rather than a grand canonical ensemble, further discussed in the conclusions. This algorithm could target mini-batch size rather than hidden layer size or target the induced mini-batch update on the parameters.

Comparing the average Fig. \ref{avg} and error Fig. \ref{acc}, there is a slight benefit for the average with the algorithm in all cases on the selection set. However, even though the effect is small in the average cross-entropy for the minimal RNNs, the improved accuracy is more than $1\%$.

The equal treatment of the different moves is not explored, and neither is the algorithm's activity. Simplifying and leaving out the relaxation could benefit these studies. 

