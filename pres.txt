Hello,
Welcome!
I was asked to do a presentation of my thesis; as it's quite technical, I decided to divide it into two parts. One introduction with a focus on the application and theory of artificial neural networks in machine learning. The other part with what I did specifically in my thesis and here at Rumbline. As you know, my supervisor was Dr. Siril Yella here at Rumbline Consulting and Prof. Egor Babaev at KTH and Emil Blomquist, who is a doctoral student at the theoretical physics department at KTH. 

Without further ado, let's get to it. Machine learning is a broad subject. Among the most prominent applications are image recognition or computer vision, email and social network filtering, fraud detection, speech recognition, medical diagnosis, playing board and video games, machine translation, and natural language processing. Machine translation is a machine translation, and a natural language is a human language that naturally evolved. They have a way with words. A few of these can be solved using linear regression and are still branded as machine learning techniques. Don't be fooled; many statistical methods overlap with the field, which creates opportunities and difficulties. However, if you apply deep learning, that is a sign that what you are doing can't be solved so easily. 

Now, how can a single methodology perform such diverse problems or tasks? If the data is properly transformed then by starting with creating a popular artificial neural network. Think of it as a model of the brain with the possibility of learning; that's where the learning in machine learning comes from. And machine, well... How this couples to the broader field of AI is that the machine shows signs of intelligence in that it can learn. But AI is a much broader field and approaches philosophy, where you answer questions like "how does a machine appear intelligent?". As the technology advances, this horizon advances in a similar manner, and what is considered proof of intelligence is constantly moved further along.

Now, how this model "learns" is quite straightforward if you remember high school biology, from the impulse an input is generated. Then the neurons fire and propagate the signal through the network, and in the end, an output is created. The output is then compared to a correct answer or target output. Here is a figure of a neural network represented as a graph.

In more detail, artificial neural networks usually consist of three layers of neurons called nodes, an input layer, a hidden, and an output layer. As their name suggests, their function differs slightly. The hidden layer is where the computation takes place, and the number of hidden layers determines the depth of the network. The other two layers' functions are straightforward and mainly controlled by the data set and task. In between layers and nodes are connections. They are directed and point from the input layer towards the output layer. In a fully connected neural network, all nodes in the previous layer connect to all nodes in the next layer. 

The nodes all mirror a strength called bias, and all connections mirror a strength called weights. These biases and weights together fully represent the network and are its set of parameters. If a node gets a signal, an activation function like the sigmoid is used to determine the node's signal. This is where the non-linear property of the artificial neural network stems from. 

Each layer consists of nodes, the number of input nodes corresponding to the number of input variables. The number of output nodes to the number of outputs, for example, in a classification task corresponding to the number of classes. The hidden layer is variable and heuristically set. There is no exact science to decide on the number of hidden nodes. However, the number of hidden layers is set by the sought complexity to solve the problem. Here are the requirements, I don't want you to read and fully understand but just to give you an idea.

After creating an output, there could be a possible error, a mismatch to the target, and this is sent back through the network to adjust for the next time such that the neural network gets it right or at least more right the next time. This is called supervised learning, where the right answer is known. For artificial neural networks, the training is done by considering a lot of examples. And I mean a lot of examples. Doing this procedure over and over ten thousands of times, this is called training. They say you become a master after training for 10 thousand hours; this applies to neural networks. However, we are talking about 10 hours for neural networks, which is still a long time for a computer program. 

Normally, training is done by stochastic gradient descent. The stochastic part is the property that the gradient is evaluated on a random subset of the whole data set and thus produces a random correction to the cost function. The cost function is a way of keeping track of performance. The gradient can be thought of as the derivative of the neural network evaluated at the subset, called mini-batch. Usually, you call the neural network a black box; this is just black box prime if you remember calculus. So we only have access to inquiries and can then evaluate the black box and its derivative.

The way the neural network gets it more right with the derivative is by tuning parameters during training. Each node and connection are related to the strength of a signal sent by it and can be adjusted. This is done by considering the error sent back through the neural network and changing the signal's strength by a proportionate amount to the strength and the error. This is called backpropagation, and it's that simple. The black box prime was surprisingly easy. However, to elaborate on the black box, it is such that the function which the neural net approximates or tries to imitate does not have a closed-form in the usual case. For linear regression, the closed-form is used as a guess; this is a big difference between the two methods. By keeping the form abstract, it is possible to for it to develop on its own.

In machine learning and specifically deep learning, two popular models are Recurrent neural networks and Convolutional nets. The Convolutional Neural Networks have special layers that perform operations to construct more complex patterns from simpler patterns using a hierarchical structure and the spatial data's local correlation. The data is then sent through a fully connected layer. This is very beneficial for analyzing pictures, as their amount of data scales badly for fully connected architectures. 

The other model is the recurrent neural network, there are of course variants of this, the most popular being the LSTM. The basic recurrent neural network is the architecture we used in the thesis and generalizes from the fully connected model by allowing for recursion. Typically they look as in the picture, with one hidden layer. As you can see, there is a time axis forming along when the network is unfolded. The idea is that the dynamics allow for memory in the hidden state and allow the network to process variable-length sequences of input. This is good for speech recognition. However, the model cannot capture long temporal dependencies. The crux is that for the necessary requirement for the model to robustly store information, is also a sufficient condition for it to forget.

This is where the Hessian-free optimization method comes in. Remember the black box prime? This is the black box prime 2.0. It is a second-order method employing the second derivative of the neural network, called the Hessian. To get the method to work completely, additional measures have to be taken. In a seminal paper for the training of recurrent neural networks, this method's strength and procedure are presented. Basically, it overturned dying interest in the recurrent structure. However, the method prolongs the training by a large factor even though it gets better results. 

The reason for relying on a second-order method is that when calculating the derivative, you take a step in its opposite direction to update all parameters simultaneously. This is because the goal is to minimize the cost; dangers here lie in local minima. These are areas where optimization performs badly and believes to be done. But how big should this step be? With the second-order method, this is computed using the conjugate gradient method, optimizing an incremental equation between derivative and the Hessian. This was applied in the thesis to assure success in the training of the recurrent neural network.

Before we start with the thesis method, I should introduce the concept of overfitting. Statistical methods rely on parameters to describe data. If there are too many, than justifiable, the model will adhere to the regarded data better than necessary but fail to fit additional data. Likewise, it can underfit. The overfitting concept translates to machine learning as overtraining. Here the problem would be if the model accurately fits a mini-batch but loses generalization on the test set. And we see this could be the case for the whole training set. What is done is the training set is divided. So the training set divides into training and selection. The test set, of course, cannot be touched.

Commonly called regularization, these are methods used to combat overfitting. This is normally done by introducing penalizing terms in the cost function to limit suggested parameter updates' complexity. This is already a part of the Hessian-free approach. Another regularization is the reset of parameters to zero or deletion, called pruning. As mentioned, too many parameters lead to overfitting. This also makes training faster, as there are fewer parameters to train. One technique is called dropout and consists of dropping out nodes of the network. This generates a lot of networks that are parts of the whole and can be trained in parallel.

Now for the method, we studied in my thesis, Monte Carlo Regularization. It's one of looking at the neural network structure and deciding if you should add or subtract parts of it, perturbating the structure. How this is done is simply by calculating the effect of performing a perturbation of the structure and benchmarking it. However, this cannot be done on the test data set. Like mentioned, what is done is you divide the training data into a training set and selection set. The selection set is then used for benchmarking. Deciding on categories of perturbations and then selecting the best in each category, rejecting if they are negative. A random one is chosen out of these updates, as no guarantee exists that the best out of the best will minimize the test set error.

The data set we used was the MNIST data set, consisting of handwritten numbers zero to nine. Here the task is to classify the numbers by the neural network looking at the picture. Of course, it is not obvious how this translates to a task for a recurrent neural network designed to handle sequences. One could input one pixel at a time, there are 28x28 pixels in each picture. But what we did was input one row of pixels, 28 pixels, at a time. By doing so the memory of the recurrent neural networks is used, as mentioned there is a limit, at around a 100 timesteps for which it can no longer remember.

Now looking at the results. First is a figure of the selection set on the cross-entropy cost, which is often used in multiclass classification. The first run through the data set is omitted in the figure as the cross-entropy is ten times higher at the start when initialized. You can see the largest structure of the initial hidden layer size 67 towards the bottom. As expected, larger nets perform better. There is a lot of data in this picture, and the reason is the mini-batching, dividing the training set into smaller pieces gives high variance. The Hessian-free approach allows for larger mini-batches, normally 100 samples are used, but here a 1000 samples were used at a time.

Instead, looking at the average, things clear upâ€”the average over the cross-entropy on the current mini-batch and the average on the selection set. The largest performance boost in the average is seen in the middle, both on the local mini-batch and selection set. On the edges, it does not seem so beneficial for the smallest and biggest. The selection set decrease in average suggests the method allows for better generalization. 

Finally, a figure of the accuracy or rather the error on the test set, after all training is done. Here we see benefit even for the smallest hidden layer. However, the largest does not benefit. This would require a scaling mechanism; the algorithm only perturbs in a fixed-sized manner even though the network grows. Here the error summarized in a table.

So to summarize, the regularization seems to work even though we used advanced optimization, with its own regularization. As can be seen, as much as possible has been done for the regularization to be applicable to any kind of neural network. Effectitvely relying on the cost function and selection set, to benchmark arbitrary parameter. This was done in C++ without any library or aid. This was a statistical physics inspired work and future studies could be performed on concepts similar to  inspire for other methods.