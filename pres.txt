Hello,
Welcome!
I was asked to do a presentation of my thesis, as it's quite technical I decided to divide it into two parts. One introduction with focus on application and theory of artificial neural networks in machine learning. The other part with what I did specifically in my thesis and here at Rumbline. As you know, my supervisor was Dr. Siril Yella here at Rumbline Consulting and Emil Blomquist, who is a doctoral student at the theoretical physics department at KTH. 

Without further ado, lets get to it. Machine learning is a broad subject and among the most prominent fields of application are image recognition or computer vision, email and social network filtering, fraud detection, speech recognition, medical diagnosis, playing board and video games, machine translation and natural language processing. Where machine translation is translation done by a machine and a natural language is a human language, naturally evolved. They have a way with words. A few of these can be solved by for example linear regression, and are still branded as machine learning techniques. Don't be fooled, a lot of statistical methods overlap with the field and this creates oppurtunities. However, if you apply deep learning that is a sign that it can't be solved so easily. 

Now, how can such diverse problems or tasks be performed by a single methodology? By starting with creating a popular artificial neural network. Think of it like a model of the brain with the possibility of learning, that's where the learning in machine learning comes from. And machine, well... How this couples to the broader field of AI is that the machine shows signs of intelligence in that it can learn. But AI is a much broader field and approaches philosophy, where you answer questions like "how does a machine appear intelligent?". As the technology advances this horizon advances in a similar manner and what is considered proof of intelligence is constantly moved further along.

Now, how this model "learns" is quite straighforward if you remember high school biology, from the impulse an input is generated. Then the neurons fire and propagate the signal through the network and at the end an output is created. The output is then compared to a correct answer or target output. 

In more detail, artificial neural networks usually consist of three layers of neurons called nodes, an input, a hidden and an output layer. As their name suggest, their function differ slightly. The hidden layer is where the computation takes place and the number of hidden layers determines the depth of the network. The other two layer's functions are straighforward and are mainly controlled by the data set and task at hand. In between layers and nodes are connections. They are directed and point from the input layer towards the output layer. In a fully connected neural network, all nodes in the previous layer connect to all nodes in the next layer. 

The nodes all mirror a strength called bias and all connections mirror a strength called weights. These biases and weights together fully represent the network and are its set of parameters. If a node gets a signal, an activation function like the sigmoid is used to determine the signal out of the node. This is where the non-linear property of the artificial neural network stems from. 

Each layer consists of nodes, the number of input nodes corresponding to the input. The number of output nodes to the number of outputs, for example in a classification task corresponding to the number of classes. The hidden layer is variable and heuristically set. There is no exact science to decide on the number of hidden nodes. However, number of hidden layers are set by the sought complexity to solve the problem.

After creating an output there could be a possible error and this is sent back through the network, to adjust for the next time such that the neural network gets it right or at least more right the next time. This is called supervised learning, where the right answer is known. For artificial neural networks the training is done by considering a lot of examples. And I mean a lot of examples. Doing this procedure over and over 10 thousands of times, this is called training. They say you become a master after training for 10 thousand hours, this is applicable to neural networks, however, for neural networks we are talking about 10 hours. Which is still a long time for a computer program. 

Normally, training is done by stochastic gradient descent. The stochastic part is the property that the gradient is evaluated on a random subset of the whole data set and thus produces a random correction. Think of it as a derivative of the neural network evaluated at the subset, called mini-batch. Usually you call the neural network a black box, this is just black box prime if you remember calculus. So we only have access to inquires and are able to then evaluate the black box and its derivative.

The way the neural network gets it more right with the derivative is by tuning parameters during training. Each node and connection relate to a strength of a signal sent through it and can be adjusted. This is done by considering the error sent back through the neural network and changing the signal by a proportionate amount to the strength and the error. This is called backpropagation and it's that simple. Black box prime was sursprisingly easy. However, to elaborate on the black box it is such that the function which the neural net approximates or tries to imitate does not have a closed form in the usual case. For linear regression, the closed form is used as a guess, this is a big difference between the two methods. By keeping the form unknown it is possible to develop on its own.

In machine learning and specifically deep learning, two popular models are Recurrent neural networks and Convolutional Neural Networks. The Convolutional Neural Networks have special layers that perform operations to construct more complex patterns from simpler patterns using hierarchical structure, the local correlation of the spatial data. The data is then sent through a fully connected layer. This is very beneficial for analysing pictures, as their amount of data scales badly for fully connected architectures. 

The other model is the recurrent neural network. This is the architecture I used in my thesis, and generalizes from the fully connected model by allowing for recursion. Typically they look as in the picture, with one hidden layer. As you can see there is a time axis forming when the network is unfolded. The idea is that the dynamics allow for memory in the hidden state and allows for the network to process variable length sequences of input. This is good for speech recognition. However, the model cannot capture long temporal dependencies. The crux is that for the model to robustly store information that is also a sufficient condition for it to forget.

This is where the Hessian-free optimization method comes in. Remember the black box prime? This is the black box prime 2.0. It is a second order method employing the second derivative of the neural network, called the Hessian. To get it to work properly, additional measures have to be taken. In a seminal paper for training of recurrent neural networks, the strength and procedure of this method is presented. Basically it overturned dying interest in the recurrent structure. However, the method prolongs the training by a large factor even though it gets better results. 

The reason for relying on a second order method is that when calculating the derivative, you take a step in its opposite direction to update all parameters simultaneously. But how big should this step be? With the second order method this is computed using the conjugate gradient method, optimizing a incremental equation between derivative and the Hessian. This was applied in the thesis to assure succes in training of the recurrent neural network.

Now for the method, Monte Carlo Regularization. Its one of looking at the structure of the neural network and deciding if you should add or subtract parts of it, perturbating the structure. How this is done is simply by calculating the effect of performing a perturbation of the structure and benchmarking it. However, this cannot be done on the test data set. So what is done is you divide the training data, into training set and selection set. The selection set is then used for benchmarking. Deciding on categories of perturbations and then selecting the best in each category, rejecting if they are negative. Out of these updates, a random one is chosen, as no guarantee exists that the best out of the best will minimize the test set error.

Now looking at the results. First is a figure of the selection set the cross-entropy energy, which is often used in multiclass classification. The first run through the data set is omitted as the cross-entropy is 10 times higher at the start. You can see the largest structure of initial hidden layer size 67 towards the bottom. There is a lot of data in this picture and the reason is the mini-batching, dividing the training set into smaller pieces gives high variance. The Hessian-free approach allows for larger mini-batches, normally 100 samples is used but here a 1000 samples were used.

Instead looking at the average, things clear up. The average over the cross-entropy on the current mini-batch and the average on the selection set. In the middle the largest performance boost in the average is seen, both on the local mini-batch and selection set. On the edges it does not seem so beneficial, for the smallest and biggest. The selection set decrease in average suggests the method allows for better generalization. 

Finally a figure of the accuracy or rather the error on the test set, after all training is done. Here we see benefit even for the smallest. However, the largest does not benefit. This would require a scaling mechanism, the algorithm right now only perturbs in a fixed size manner even though the network grows. Here the last plot in a table.

